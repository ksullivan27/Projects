{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qs0yamHN-dY0"
      },
      "source": [
        "# **CIS 5300 Fall 2022 - Homework 3 - Sentiment Analysis with Deep Neural Networks**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lp00SIIw7xRi"
      },
      "source": [
        "In this assignmet you will use Pytorch, a popular deep learning library, to create sentiment classifiers using some of the models we have talked about in class. The goal of the assignment is to familiarize yourself with Pytorch, and a few of the issues specific to text processing using neural networks on GPUs. We will use Sentiment Analysis as an example classification problem, and consider a simple binary version of the problem. You will need to implement classifiers using Deep Averaging Networks and different Recurrent Neural Networks (i.e. LSTMs). Finally, you will need to make a thorough comparison of these approaches, and do a qualitative analysis of the errors.\n",
        "\n",
        "This assignment consists of two parts. Completing the coding section will help you to complete the report:\n",
        "\n",
        "1.   (75 points) A coding portion within this notebook, with test cases evaluated through PennGrader.\n",
        "2.   (75 points) A written report asking you to evaluate and analyze variants of the systems you design. Please answer all questions within the the following template: https://www.overleaf.com/read/sjtwwtnptzzx\n",
        "\n",
        "Please read the written report requirements before you start the coding portion.\n",
        "\n",
        "In the past, we introduced a version of this assignment for 5190. You will be asked to go much further here but if you are familiar with parts of this assignment from that class, you may reuse code.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekiXC4jh2MjA"
      },
      "source": [
        "## Setup 1. PennGrader Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "LCTCgGCi2aDj",
        "ExecuteTime": {
          "end_time": "2024-02-27T11:33:15.115614Z",
          "start_time": "2024-02-27T11:33:15.111943Z"
        }
      },
      "outputs": [],
      "source": [
        "## DO NOT CHANGE ANYTHING, JUST RUN\n",
        "%%capture\n",
        "!pip install penngrader-client dill"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBqXoCTK2dYG",
        "outputId": "71d901a8-0be1-4eb0-d0b2-feef3a9b08ab",
        "ExecuteTime": {
          "end_time": "2024-02-27T11:33:16.956947Z",
          "start_time": "2024-02-27T11:33:16.952918Z"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing notebook-config.yaml\n"
          ]
        }
      ],
      "source": [
        "%%writefile notebook-config.yaml\n",
        "\n",
        "grader_api_url: 'https://23whrwph9h.execute-api.us-east-1.amazonaws.com/default/Grader23'\n",
        "grader_api_key: 'flfkE736fA6Z8GxMDJe2q8Kfk8UDqjsG3GVqOFOa'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iwrElnxa2gBe",
        "outputId": "700c740c-7d3f-4f03-c758-1ab93c2793bc",
        "ExecuteTime": {
          "end_time": "2024-02-27T11:33:23.792383Z",
          "start_time": "2024-02-27T11:33:23.664435Z"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "grader_api_url: 'https://23whrwph9h.execute-api.us-east-1.amazonaws.com/default/Grader23'\n",
            "grader_api_key: 'flfkE736fA6Z8GxMDJe2q8Kfk8UDqjsG3GVqOFOa'\n"
          ]
        }
      ],
      "source": [
        "!cat notebook-config.yaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RABF5GNZ2hsN",
        "outputId": "7b26be22-a7fe-40d5-f9c2-803722416dab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PennGrader initialized with Student ID: 37824171\n",
            "\n",
            "Make sure this correct or we will not be able to store your grade\n"
          ]
        }
      ],
      "source": [
        "from penngrader.grader import *\n",
        "\n",
        "## TODO - Start\n",
        "STUDENT_ID = 'XXXXXXXX' # YOUR PENN-ID GOES HERE AS AN INTEGER#\n",
        "## TODO - End\n",
        "\n",
        "SECRET = STUDENT_ID\n",
        "grader = PennGrader('notebook-config.yaml', 'cis5300_fall_2023_HW3', STUDENT_ID, SECRET)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eQKtk_l-2nBU"
      },
      "outputs": [],
      "source": [
        "def reload_grader():\n",
        "    grader = PennGrader('notebook-config.yaml', 'cis5300_fall_2023_HW1', STUDENT_ID, SECRET)\n",
        "    return grader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m11OcgPi2pXQ",
        "outputId": "751a4206-d4cb-4cd4-f932-397e48eab8a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correct! You earned 1/1 points. You are a star!\n",
            "\n",
            "Your submission has been successfully recorded in the gradebook.\n"
          ]
        }
      ],
      "source": [
        "# check if the PennGrader is set up correctly\n",
        "# do not change this cell, see if you get 1/1!\n",
        "name_str = 'XXXXXXXX'\n",
        "grader.grade(test_case_id = 'name_test', answer = name_str)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CCFKR6Odl00"
      },
      "source": [
        "## Setup 2: Dataset/Packages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4WVrNKQC6nQ"
      },
      "source": [
        "Importing packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rb-WLp5Z-cdy",
        "outputId": "a1646cbd-46b0-437f-da86-82bcf1d33567",
        "ExecuteTime": {
          "end_time": "2024-02-27T11:33:31.003521Z",
          "start_time": "2024-02-27T11:33:27.085170Z"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wget\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9655 sha256=da6e9ecf58829ee9431e4c091d948cbb458f30d88d36581e5e844f0a1db7c701\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/f1/7f/5c94f0a7a505ca1c81cd1d9208ae2064675d97582078e6c769\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ]
        }
      ],
      "source": [
        "#@title\n",
        "from __future__ import division\n",
        "import os, sys, re, json, time, datetime, shutil\n",
        "import itertools, collections\n",
        "from importlib import reload\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import sys\n",
        "import matplotlib.pyplot as plt\n",
        "from numpy.linalg import *\n",
        "np.random.seed(42)  # don't change this line\n",
        "\n",
        "import base64\n",
        "\n",
        "# NLTK, NumPy, and Pandas.\n",
        "import nltk\n",
        "from nltk.tree import Tree\n",
        "from numpy import random as rd\n",
        "import random\n",
        "\n",
        "import collections\n",
        "import re\n",
        "import time\n",
        "import itertools\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "import glob\n",
        "from argparse import ArgumentParser\n",
        "\n",
        "#Pytorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "!pip3 install wget"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ma4E29Xe8Hs"
      },
      "source": [
        "Downloading Required files\n",
        "\n",
        "[train parquet file](https://www.cis.upenn.edu/~myatskar/teaching/cis519/a5/train.parquet)\n",
        "\n",
        "[dev parquet file](https://www.cis.upenn.edu/~myatskar/teaching/cis519/a5/dev.parquet)\n",
        "\n",
        "[test parquet file](https://www.cis.upenn.edu/~myatskar/teaching/cis519/a5/test.parquet)\n",
        "\n",
        "[tokens in training data](https://www.cis.upenn.edu/~myatskar/teaching/cis519/a5/train_tokens.txt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LumyzFXKpDIa",
        "outputId": "765940f2-d887-4886-80f4-bad3a2f39f93",
        "ExecuteTime": {
          "end_time": "2024-02-27T11:33:35.480080Z",
          "start_time": "2024-02-27T11:33:34.725426Z"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-02-27 11:50:36--  https://www.cis.upenn.edu/~myatskar/teaching/cis519/a5/train_tokens.txt\n",
            "Resolving www.cis.upenn.edu (www.cis.upenn.edu)... 158.130.69.163, 2607:f470:8:64:5ea5::d\n",
            "Connecting to www.cis.upenn.edu (www.cis.upenn.edu)|158.130.69.163|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 886453 (866K) [text/plain]\n",
            "Saving to: ‘train_tokens.txt’\n",
            "\n",
            "train_tokens.txt    100%[===================>] 865.68K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2024-02-27 11:50:36 (8.67 MB/s) - ‘train_tokens.txt’ saved [886453/886453]\n",
            "\n",
            "--2024-02-27 11:50:36--  https://www.cis.upenn.edu/~myatskar/teaching/cis519/a5/train.parquet\n",
            "Resolving www.cis.upenn.edu (www.cis.upenn.edu)... 158.130.69.163, 2607:f470:8:64:5ea5::d\n",
            "Connecting to www.cis.upenn.edu (www.cis.upenn.edu)|158.130.69.163|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2893816 (2.8M)\n",
            "Saving to: ‘train.parquet’\n",
            "\n",
            "train.parquet       100%[===================>]   2.76M  17.9MB/s    in 0.2s    \n",
            "\n",
            "2024-02-27 11:50:37 (17.9 MB/s) - ‘train.parquet’ saved [2893816/2893816]\n",
            "\n",
            "--2024-02-27 11:50:37--  https://www.cis.upenn.edu/~myatskar/teaching/cis519/a5/dev.parquet\n",
            "Resolving www.cis.upenn.edu (www.cis.upenn.edu)... 158.130.69.163, 2607:f470:8:64:5ea5::d\n",
            "Connecting to www.cis.upenn.edu (www.cis.upenn.edu)|158.130.69.163|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 317137 (310K)\n",
            "Saving to: ‘dev.parquet’\n",
            "\n",
            "dev.parquet         100%[===================>] 309.70K  --.-KB/s    in 0.09s   \n",
            "\n",
            "2024-02-27 11:50:37 (3.28 MB/s) - ‘dev.parquet’ saved [317137/317137]\n",
            "\n",
            "--2024-02-27 11:50:37--  https://www.cis.upenn.edu/~myatskar/teaching/cis519/a5/test.parquet\n",
            "Resolving www.cis.upenn.edu (www.cis.upenn.edu)... 158.130.69.163, 2607:f470:8:64:5ea5::d\n",
            "Connecting to www.cis.upenn.edu (www.cis.upenn.edu)|158.130.69.163|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 850723 (831K)\n",
            "Saving to: ‘test.parquet’\n",
            "\n",
            "test.parquet        100%[===================>] 830.78K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2024-02-27 11:50:37 (7.44 MB/s) - ‘test.parquet’ saved [850723/850723]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#@title\n",
        "!wget  -c  https://www.cis.upenn.edu/~myatskar/teaching/cis519/a5/train_tokens.txt\n",
        "!wget  -c  https://www.cis.upenn.edu/~myatskar/teaching/cis519/a5/train.parquet\n",
        "!wget  -c  https://www.cis.upenn.edu/~myatskar/teaching/cis519/a5/dev.parquet\n",
        "!wget  -c  https://www.cis.upenn.edu/~myatskar/teaching/cis519/a5/test.parquet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AG1RbvIgdCuz"
      },
      "source": [
        "If the cells above fails to download all the files, rerun a couple of times or download them and add them manually."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "v0JC3vd0cvfG",
        "ExecuteTime": {
          "end_time": "2024-02-27T11:33:38.381616Z",
          "start_time": "2024-02-27T11:33:38.374966Z"
        }
      },
      "outputs": [],
      "source": [
        "train_file = \"train.parquet\"\n",
        "dev_file = \"dev.parquet\"\n",
        "test_file = \"test.parquet\"\n",
        "vocab_file = \"train_tokens.txt\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZuEpzagkVzlZ"
      },
      "source": [
        "## 1. Introduction to Pytorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkyB5UDOWhsO"
      },
      "source": [
        "PyTorch is an optimized tensor library for deep learning using GPUs and CPUs. This homework will be using PyTorch to build Neural Networks to solve an NLP classification problem.\n",
        "\n",
        "If you are new to using the framework it is worthwhile to spend an hour reviewing the concepts of tensors, autograd, training classifiers etc in [this tutorial](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html).\n",
        "\n",
        "Refer to [this tutorial](https://pytorch.org/tutorials/beginner/introyt/modelsyt_tutorial.html) to learn more about how to build models with Pytorch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LcrJaJ73Rk9v"
      },
      "source": [
        "## 2. Stanford Sentiment Treebank(SST)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yp9rryptRoLR"
      },
      "source": [
        "We'll be using [Stanford Sentiment Treebank](https://nlp.stanford.edu/sentiment/index.html) (SST) dataset, a dataset introduced by [(Socher et al. 2013)](http://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf) consisting of 11,855 sentences drawn from a corpus of movie reviews (originally from Rotten Tomatoes), each labeled with sentiment on a five-point scale.\n",
        "\n",
        "An example of the five-point scale is:\n",
        "```\n",
        "sentence: [A warm , funny , engaging film .]\n",
        "label:    4 (very positive)\n",
        "```\n",
        "\n",
        "\n",
        "**Note:** Unlike most classification datasets, SST is also a _treebank_, which means each sentence is associated with a tree structure that decomposes it into subphrases. So for the example above, we'd also have sentiment labels for `[warm , funny]` and `[engaging film .]` and so on. The tree structure will comes in handy for complex NLP tasks and we will be using it briefly to analyze an example that has negation. The data is distributed as serialized trees in [S-expression](https://en.wikipedia.org/wiki/S-expression) form, like this:\n",
        "```\n",
        "(4 (4 (2 A) (4 (3 (3 warm) (2 ,)) (3 funny))) (3 (2 ,) (3 (4 (4 engaging) (2 film)) (2 .))))\n",
        "```\n",
        "Generally, we can exploit these richer annotations when training a classifier by also using phrases as training instances. This augmented training signal is important for getting good performance for models that have many parameters.\n",
        "\n",
        "We've downloaded the dataset and parse the S-expressions into a dataframe. Run the remaining cells in Section 2 to setup the dataset.\n",
        "\n",
        "\n",
        "***In this assignment, we simplified the task from multi-class classification to binary classification. The label in the dataset would be either 0 (negative sentiment) or 1 (positive sentiment)***\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "olCRGZcIJuEf"
      },
      "source": [
        "### 2.1 Parsing Tree Structure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "fDhs_mpQAQIt",
        "ExecuteTime": {
          "end_time": "2024-02-27T11:33:42.042348Z",
          "start_time": "2024-02-27T11:33:42.026894Z"
        }
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "class SSTDataset(object):\n",
        "    def __init__(self, train_file, dev_file, test_file, vocab_file, V=20000):\n",
        "        self.vocab = None\n",
        "        self.train = pd.read_parquet(train_file)\n",
        "        self.dev = pd.read_parquet(dev_file)\n",
        "        self.test = pd.read_parquet(test_file)\n",
        "        train_words =[]\n",
        "        with open(vocab_file) as f:\n",
        "            train_words = f.readlines()\n",
        "        train_words = [w.strip() for w in train_words]\n",
        "        # Build vocabulary over training set\n",
        "        self.vocab = Vocabulary(train_words, size=V)\n",
        "        print(\"Train set has {:,} words\".format(self.vocab.size))\n",
        "\n",
        "    def get_filtered_split(self, split='train',is_root = True):\n",
        "        df = getattr(self, split)\n",
        "        if is_root:\n",
        "            df = df[df.is_root]\n",
        "        return df\n",
        "\n",
        "    def as_padded_array(self, split='train', max_len=40, pad_id=0,is_root = True):\n",
        "        df = self.get_filtered_split(split,is_root)\n",
        "        x, ns = pad_np_array(df.ids, max_len=max_len, pad_id=pad_id)\n",
        "        y = np.empty((1,1))\n",
        "        if split != 'test':\n",
        "            y  = np.array(df.label, dtype=np.int32)\n",
        "        return x, ns, y\n",
        "\n",
        "    def as_sparse_bow(self, split='train',is_root = True):\n",
        "        from scipy import sparse\n",
        "        df = self.get_filtered_split(split,is_root)\n",
        "        x = id_lists_to_sparse_bow(df['ids'], self.vocab.size)\n",
        "        if split != 'test':\n",
        "            return x, np.array(df.label, dtype=np.int32)\n",
        "        return x\n",
        "\n",
        "    Example_fields = [\"tokens\", \"ids\", \"label\", \"is_root\", \"root_id\"]\n",
        "    Example = collections.namedtuple(\"Example\", Example_fields)\n",
        "\n",
        "    def canonicalize(self, raw_tokens):\n",
        "        wordset=(self.vocab.wordset if self.vocab else None)\n",
        "        return canonicalize_words(raw_tokens, wordset=wordset)\n",
        "\n",
        "# Constants for use by other modules.\n",
        "START_TOKEN = u\"<s>\"\n",
        "END_TOKEN   = u\"</s>\"\n",
        "UNK_TOKEN   = u\"<unk>\"\n",
        "\n",
        "def require_package(package_name):\n",
        "    import pkgutil\n",
        "    import subprocess\n",
        "    import sys\n",
        "    if not pkgutil.find_loader(package_name):\n",
        "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', package_name])\n",
        "\n",
        "def canonicalize_digits(word):\n",
        "    if any([c.isalpha() for c in word]): return word\n",
        "    word = re.sub(\"\\d\", \"DG\", word)\n",
        "    if word.startswith(\"DG\"):\n",
        "        word = word.replace(\",\", \"\") # remove thousands separator\n",
        "    return word\n",
        "\n",
        "def canonicalize_word(word, wordset=None, digits=True):\n",
        "    word = word.lower()\n",
        "    if digits:\n",
        "        if (wordset != None) and (word in wordset): return word\n",
        "        word = canonicalize_digits(word) # try to canonicalize numbers\n",
        "    if (wordset == None) or (word in wordset):\n",
        "        return word\n",
        "    else:\n",
        "        return UNK_TOKEN\n",
        "\n",
        "def canonicalize_words(words, **kw):\n",
        "    return [canonicalize_word(word, **kw) for word in words]\n",
        "\n",
        "\n",
        "def pad_np_array(example_ids, max_len=250, pad_id=0):\n",
        "    arr = np.full([len(example_ids), max_len], pad_id, dtype=np.int32)\n",
        "    ns = np.zeros([len(example_ids)], dtype=np.int32)\n",
        "    for i, ids in enumerate(example_ids):\n",
        "        cpy_len = min(len(ids), max_len)\n",
        "        arr[i,:cpy_len] = ids[:cpy_len]\n",
        "        ns[i] = cpy_len\n",
        "    return arr, ns\n",
        "\n",
        "def id_lists_to_sparse_bow(id_lists, vocab_size):\n",
        "    from scipy import sparse\n",
        "    ii = []  # row indices (example ids)\n",
        "    jj = []  # column indices (token ids)\n",
        "    for row_id, ids in enumerate(id_lists):\n",
        "        ii.extend([row_id]*len(ids))\n",
        "        jj.extend(ids)\n",
        "    x = sparse.csr_matrix((np.ones_like(ii), (ii, jj)),\n",
        "                          shape=[len(id_lists), vocab_size])\n",
        "    return x\n",
        "\n",
        "class Vocabulary(object):\n",
        "\n",
        "    START_TOKEN = START_TOKEN\n",
        "    END_TOKEN   = END_TOKEN\n",
        "    UNK_TOKEN   = UNK_TOKEN\n",
        "\n",
        "    def __init__(self, tokens, size=None,\n",
        "                 progressbar=lambda l:l):\n",
        "        self.unigram_counts = Counter()\n",
        "        self.bigram_counts = defaultdict(lambda: Counter())\n",
        "        prev_word = None\n",
        "        for word in progressbar(tokens):  # Make a single pass through tokens\n",
        "            self.unigram_counts[word] += 1\n",
        "            self.bigram_counts[prev_word][word] += 1\n",
        "            prev_word = word\n",
        "        self.bigram_counts.default_factory = None  # make into a normal dict\n",
        "\n",
        "        # Leave space for \"<s>\", \"</s>\", and \"<unk>\"\n",
        "        top_counts = self.unigram_counts.most_common(None if size is None else (size - 3))\n",
        "        vocab = ([self.START_TOKEN, self.END_TOKEN, self.UNK_TOKEN] +\n",
        "                 [w for w,c in top_counts])\n",
        "\n",
        "        # Assign an id to each word, by frequency\n",
        "        self.id_to_word = dict(enumerate(vocab))\n",
        "        self.word_to_id = {v:k for k,v in self.id_to_word.items()}\n",
        "        self.size = len(self.id_to_word)\n",
        "        if size is not None:\n",
        "            assert(self.size <= size)\n",
        "\n",
        "        # For convenience\n",
        "        self.wordset = set(self.word_to_id.keys())\n",
        "\n",
        "        # Store special IDs\n",
        "        self.START_ID = self.word_to_id[self.START_TOKEN]\n",
        "        self.END_ID = self.word_to_id[self.END_TOKEN]\n",
        "        self.UNK_ID = self.word_to_id[self.UNK_TOKEN]\n",
        "\n",
        "    def words_to_ids(self, words):\n",
        "        return [self.word_to_id.get(w, self.UNK_ID) for w in words]\n",
        "\n",
        "    def ids_to_words(self, ids):\n",
        "        return [self.id_to_word[i] for i in ids]\n",
        "\n",
        "    def ordered_words(self):\n",
        "        \"\"\"Return a list of words, ordered by id.\"\"\"\n",
        "        return self.ids_to_words(range(self.size))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wcs9j-CDDcFC"
      },
      "source": [
        "### 2.2 Brief Look at SSTDataset Functionality\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CSRZ9eqxt6-b",
        "outputId": "da200fcc-24b8-47b2-eb7b-41c93b439b31",
        "ExecuteTime": {
          "end_time": "2024-02-27T11:33:45.736660Z",
          "start_time": "2024-02-27T11:33:45.539671Z"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train set has 16,474 words\n"
          ]
        }
      ],
      "source": [
        "ds = SSTDataset(train_file, dev_file, test_file, vocab_file, V=20000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kY5JE1DQt6-d"
      },
      "source": [
        "A few members of the `SSTDataset()` class that we will be using are:\n",
        "- **`ds.vocab`**: a `vocabulary.Vocabulary` object managing the model vocabulary.\n",
        "- **`ds.{train,dev,test}`**: a Pandas DataFrame containing the _processed_ examples, including all subphrases. `label` is the target label, `is_root` denotes whether this example is a root node (full sentence), and `tokens` are the tokenized words from the original sentence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16q8ruRjt6-k"
      },
      "source": [
        "Note if you set `root_only=True` the dataframe will return only examples corresponding to whole sentences. If you set `root_only=False` the dataframe will return examples for all phrases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TQQOQInsv04d",
        "outputId": "6f860d3e-4541-4892-af7b-68593b8be07d",
        "ExecuteTime": {
          "end_time": "2024-02-27T11:33:47.834240Z",
          "start_time": "2024-02-27T11:33:47.823462Z"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0         [4, 606, 10, 3416, 9, 26, 4, 2821, 1263, 11, 1...\n",
              "4         [10, 3416, 9, 26, 4, 2821, 1263, 11, 108, 63, ...\n",
              "5         [10, 3416, 9, 26, 4, 2821, 1263, 11, 108, 63, ...\n",
              "7         [3416, 9, 26, 4, 2821, 1263, 11, 108, 63, 5543...\n",
              "25                                                    [108]\n",
              "                                ...                        \n",
              "318552                                       [16473, 4, 17]\n",
              "318573                               [14, 20, 507, 4161, 3]\n",
              "318577                                       [507, 4161, 3]\n",
              "318579                                            [4161, 3]\n",
              "318580                                               [4161]\n",
              "Name: ids, Length: 98794, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "ds.train['ids']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "9SXUCz-2YeP0",
        "ExecuteTime": {
          "end_time": "2024-02-27T11:33:48.561199Z",
          "start_time": "2024-02-27T11:33:48.557605Z"
        }
      },
      "outputs": [],
      "source": [
        "is_root = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2EMoXKcUQ1G"
      },
      "source": [
        "## 3. [Deep Averaging Networks](https://people.cs.umass.edu/~miyyer/pubs/2015_acl_dan.pdf)\n",
        "\n",
        "![dan](https://miro.medium.com/max/904/1*0LezMYWUk3pXptoMdO5M_Q.png)\n",
        "\n",
        "\n",
        "Vector space models represent words using high dimensional vectors called embeddings.\n",
        "Since sentences are made up of multiple words, each with their own embeddings, we must compose embeddings together to produce a classifier.\n",
        "\n",
        "Composition functions can be orginized based on the properties they capture.\n",
        "For example, some composition functions can completely ignore sentence order.\n",
        "Composition functions can vary in complexity, trying to capture different aspects of language that may be important.\n",
        "In this assignment, we will explore two simple approaches for composing input embeddings to produce a classifier.\n",
        "\n",
        "Deep averaging networks (DAN) are an unordered model which can perform very well on sentence level classification tasks.\n",
        "While not the state-of-the-art anymore, it generally performs well, is very simple to implement and apply, and very computationally fast to execute.\n",
        "\n",
        "The archecture for a DAN is described in the figure above.\n",
        "DANs works in three simple steps:\n",
        "1. Take the vector average of the embeddings\n",
        "associated with an input sequence of tokens\n",
        "2. Pass that average through one or more feedforward layers\n",
        "3. Perform (linear) classification on the final\n",
        "layer’s representation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6S7vplWWPya4"
      },
      "source": [
        "We will walk you through the process of setting up DAN.\n",
        "You will need to implement snippets along the way and we will provide PennGrader routines to help test correctness.  You will need to implement the following elements:\n",
        "\n",
        "\n",
        "*   Configuring word embeddings (Sections 3.1-3.3).\n",
        "\n",
        "*   Implementing the neural archecture in the figure above (Sections 3.4 - 3.6)\n",
        "\n",
        "*   Training and hyper-parameter tuning the model (Section 3.7)\n",
        "\n",
        "Validating hyper-parameters is a crucial part of making any deep network work, as the models can be very sensitive. You will need to validate hyperparameters of the archecture such as depth, choice of non-linearity or dropout, as well as learning hyper-parameters. In the report, you will need to include a table with this exploration.\n",
        "\n",
        "At the end of section 3, you we will evalaute the qualitity of your DAN on test data. It is possible to get as high as 85\\% on our test data. You will be graded on a linear scale between 83 and 85%. In our experiments, performance on the validation set and test set are comparable.\n",
        "\n",
        "If you are getting non-zero credit on the PennGrader cell at the end, you are on the right track but may need to validate more hyper-parameters. Select reasonable hyper-parameter experiments, focusing on the parameters that you believe will have most impact on the final performance. Do not experiment randomly or in too fine-grained a way, as you risk overfitting the validation set.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5ulmPzRwr3T"
      },
      "source": [
        "### 3.1 [Glove Embeddings](https://nlp.stanford.edu/projects/glove/)\n",
        "We are downloading pretrained glove word vectors that has been trained on Common Crawl data, a snapshot of the whole web.\n",
        "These embeddings serve as excelent initilizations for embeddings our model needs.\n",
        "Downloading glove embeddings (This will take around 10 minutes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nT27aeXBVw01",
        "outputId": "01112581-f6fe-431f-ce26-09e1730a90a7",
        "ExecuteTime": {
          "end_time": "2024-02-27T11:38:17.172726Z",
          "start_time": "2024-02-27T11:36:20.524743Z"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-02-27 11:50:38--  https://huggingface.co/stanfordnlp/glove/resolve/main/glove.840B.300d.zip\n",
            "Resolving huggingface.co (huggingface.co)... 65.8.178.93, 65.8.178.12, 65.8.178.118, ...\n",
            "Connecting to huggingface.co (huggingface.co)|65.8.178.93|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.huggingface.co/stanfordnlp/glove/f47355dd5b267bd10f08671e513790690233c76a9ffd73aa915d78f894a8912e?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27glove.840B.300d.zip%3B+filename%3D%22glove.840B.300d.zip%22%3B&response-content-type=application%2Fzip&Expires=1709293838&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwOTI5MzgzOH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9zdGFuZm9yZG5scC9nbG92ZS9mNDczNTVkZDViMjY3YmQxMGYwODY3MWU1MTM3OTA2OTAyMzNjNzZhOWZmZDczYWE5MTVkNzhmODk0YTg5MTJlP3Jlc3BvbnNlLWNvbnRlbnQtZGlzcG9zaXRpb249KiZyZXNwb25zZS1jb250ZW50LXR5cGU9KiJ9XX0_&Signature=l7R7x7K3RBjzPgsfU-OVtzGvnfaRcA-k1NkwqKVbrHnPzonJ1p1wZYr87F9k66wUaW8GDVQQE4iFExiIvvKMtBE2HBuQLUy82PYKED5GaQfdPl0DXTrurc-R8DurxNIXQ99DNd9lJ4%7EX9M64SVlh554%7EcZY78-vxKfGpxnGhWt8AZiwpgjRakDs5xTG7YOkiE18yXaS2epLpbVlYjgvY9sN-UAjdCg0fFukDqR2%7EdWx3B-JI13YM5fpznhEN3Lw5DeKUjyYPBHOBRFkxYxX7Y0%7E52TZ9mclAlqeP1Gi5NLioBRJtQRJ2hxzgYuZvzAa9dJ4bKWPddXa9eBBcVSh5xA__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
            "--2024-02-27 11:50:38--  https://cdn-lfs.huggingface.co/stanfordnlp/glove/f47355dd5b267bd10f08671e513790690233c76a9ffd73aa915d78f894a8912e?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27glove.840B.300d.zip%3B+filename%3D%22glove.840B.300d.zip%22%3B&response-content-type=application%2Fzip&Expires=1709293838&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwOTI5MzgzOH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9zdGFuZm9yZG5scC9nbG92ZS9mNDczNTVkZDViMjY3YmQxMGYwODY3MWU1MTM3OTA2OTAyMzNjNzZhOWZmZDczYWE5MTVkNzhmODk0YTg5MTJlP3Jlc3BvbnNlLWNvbnRlbnQtZGlzcG9zaXRpb249KiZyZXNwb25zZS1jb250ZW50LXR5cGU9KiJ9XX0_&Signature=l7R7x7K3RBjzPgsfU-OVtzGvnfaRcA-k1NkwqKVbrHnPzonJ1p1wZYr87F9k66wUaW8GDVQQE4iFExiIvvKMtBE2HBuQLUy82PYKED5GaQfdPl0DXTrurc-R8DurxNIXQ99DNd9lJ4%7EX9M64SVlh554%7EcZY78-vxKfGpxnGhWt8AZiwpgjRakDs5xTG7YOkiE18yXaS2epLpbVlYjgvY9sN-UAjdCg0fFukDqR2%7EdWx3B-JI13YM5fpznhEN3Lw5DeKUjyYPBHOBRFkxYxX7Y0%7E52TZ9mclAlqeP1Gi5NLioBRJtQRJ2hxzgYuZvzAa9dJ4bKWPddXa9eBBcVSh5xA__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 108.157.162.99, 108.157.162.95, 108.157.162.27, ...\n",
            "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|108.157.162.99|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2176768976 (2.0G) [application/zip]\n",
            "Saving to: ‘glove.840B.300d.zip’\n",
            "\n",
            "glove.840B.300d.zip 100%[===================>]   2.03G  47.4MB/s    in 38s     \n",
            "\n",
            "2024-02-27 11:51:17 (54.5 MB/s) - ‘glove.840B.300d.zip’ saved [2176768976/2176768976]\n",
            "\n",
            "Archive:  glove.840B.300d.zip\n",
            "  inflating: glove.840B.300d.txt     \n",
            "total 7644528\n",
            "drwxr-xr-x 1 root root       4096 Feb 27 11:51 .\n",
            "-rw-r--r-- 1 root root        150 Feb 27 11:50 notebook-config.yaml\n",
            "drwxr-xr-x 1 root root       4096 Feb 27 11:37 ..\n",
            "drwxr-xr-x 1 root root       4096 Feb 23 14:26 sample_data\n",
            "drwxr-xr-x 4 root root       4096 Feb 23 14:26 .config\n",
            "-rw-r--r-- 1 root root     886453 Mar 21  2022 train_tokens.txt\n",
            "-rw-r--r-- 1 root root     850723 Mar 21  2022 test.parquet\n",
            "-rw-r--r-- 1 root root     317137 Mar 21  2022 dev.parquet\n",
            "-rw-r--r-- 1 root root    2893816 Mar 21  2022 train.parquet\n",
            "-rw-r--r-- 1 root root 2176768976 Aug 14  2021 glove.840B.300d.zip\n",
            "-rw-rw-r-- 1 root root 5646239124 Dec 23  2015 glove.840B.300d.txt\n"
          ]
        }
      ],
      "source": [
        "#this takes about 10 minutes to run\n",
        "!wget -nc https://huggingface.co/stanfordnlp/glove/resolve/main/glove.840B.300d.zip\n",
        "!unzip glove.840B.300d.zip\n",
        "!ls -lat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "gukyScOlS6Ri",
        "ExecuteTime": {
          "end_time": "2024-02-27T11:38:23.293158Z",
          "start_time": "2024-02-27T11:38:23.288094Z"
        }
      },
      "outputs": [],
      "source": [
        "glove_file = \"glove.840B.300d.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MpUv8X36MEru",
        "outputId": "cbfa64e2-1aab-4810-a121-36b25a16ec60",
        "ExecuteTime": {
          "end_time": "2024-02-27T11:38:24.825268Z",
          "start_time": "2024-02-27T11:38:24.741897Z"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set:   x = (98794, 40) sparse, ns=(98794,), y = (98794,)\n",
            "Validation set: x = (13142, 40) sparse, ns=(13142,), y = (13142,)\n",
            "Test set:       x = (26052, 40) sparse, ns=(26052,)\n"
          ]
        }
      ],
      "source": [
        "train_x, train_ns, train_y = ds.as_padded_array(\"train\",is_root = is_root)\n",
        "dev_x, dev_ns, dev_y = ds.as_padded_array(\"dev\",is_root = is_root)\n",
        "test_x, test_ns,_  = ds.as_padded_array(\"test\",is_root = is_root)\n",
        "\n",
        "print(\"Training set:   x = {:s} sparse, ns={:s}, y = {:s}\".format(str(train_x.shape), str(train_ns.shape),\n",
        "                                                str(train_y.shape)))\n",
        "print(\"Validation set: x = {:s} sparse, ns={:s}, y = {:s}\".format(str(dev_x.shape), str(dev_ns.shape),\n",
        "                                                str(dev_y.shape)))\n",
        "print(\"Test set:       x = {:s} sparse, ns={:s}\".format(str(test_x.shape), str(test_ns.shape)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qu-xCT7dIfwD",
        "outputId": "3666b07d-813d-40bf-f4d4-b9212bbd8058",
        "ExecuteTime": {
          "end_time": "2024-02-27T11:38:26.748756Z",
          "start_time": "2024-02-27T11:38:26.617172Z"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ", -0.082752 0.67204 -0.14987 -0.064983 0.056491 0.40228 0.0027747 -0.3311 -0.30691 2.0817 0.031819 0.013643 0.30265 0.0071297 -0.5819 -0.2774 -0.062254 1.1451 -0.24232 0.1235 -0.12243 0.33152 -0.006162 -0.30541 -0.13057 -0.054601 0.037083 -0.070552 0.5893 -0.30385 0.2898 -0.14653 -0.27052 0.37161 0.32031 -0.29125 0.0052483 -0.13212 -0.052736 0.087349 -0.26668 -0.16897 0.015162 -0.0083746 -0.14871 0.23413 -0.20719 -0.091386 0.40075 -0.17223 0.18145 0.37586 -0.28682 0.37289 -0.16185 0.18008 0.3032 -0.13216 0.18352 0.095759 0.094916 0.008289 0.11761 0.34046 0.03677 -0.29077 0.058303 -0.027814 0.082941 0.1862 -0.031494 0.27985 -0.074412 -0.13762 -0.21866 0.18138 0.040855 -0.113 0.24107 0.3657 -0.27525 -0.05684 0.34872 0.011884 0.14517 -0.71395 0.48497 0.14807 0.62287 0.20599 0.58379 -0.13438 0.40207 0.18311 0.28021 -0.42349 -0.25626 0.17715 -0.54095 0.16596 -0.036058 0.08499 -0.64989 0.075549 -0.28831 0.40626 -0.2802 0.094062 0.32406 0.28437 -0.26341 0.11553 0.071918 -0.47215 -0.18366 -0.34709 0.29964 -0.66514 0.002516 -0.42333 0.27512 0.36012 0.16311 0.23964 -0.05923 0.3261 0.20559 0.038677 -0.045816 0.089764 0.43151 -0.15954 0.08532 -0.26572 -0.15001 0.084286 -0.16714 -0.43004 0.060807 0.13121 -0.24112 0.66554 0.4453 -0.18019 -0.13919 0.56252 0.21457 -0.46443 -0.012211 0.029988 -0.051094 -0.20135 0.80788 0.47377 -0.057647 0.46216 0.16084 -0.20954 -0.05452 0.15572 -0.13712 0.12972 -0.011936 -0.003378 -0.13595 -0.080711 0.20065 0.054056 0.046816 0.059539 0.046265 0.17754 -0.31094 0.28119 -0.24355 0.085252 -0.21011 -0.19472 0.0027297 -0.46341 0.14789 -0.31517 -0.065939 0.036106 0.42903 -0.33759 0.16432 0.32568 -0.050392 -0.054297 0.24074 0.41923 0.13012 -0.17167 -0.37808 -0.23089 -0.019477 -0.29291 -0.30824 0.30297 -0.22659 0.081574 -0.18516 -0.21408 0.40616 -0.28974 0.074174 -0.17795 0.28595 -0.039626 -0.2339 -0.36054 -0.067503 -0.091065 0.23438 -0.0041331 0.003232 0.0072134 0.008697 0.21614 0.049904 0.35582 0.13748 0.073361 0.14166 0.2412 -0.013322 0.15613 0.083381 0.088146 -0.019357 0.43795 0.083961 0.45309 -0.50489 -0.10865 -0.2527 -0.18251 0.20441 0.13319 0.1294 0.050594 -0.15612 -0.39543 0.12538 0.24881 -0.1927 -0.31847 -0.12719 0.4341 0.31177 -0.0040946 -0.2094 -0.079961 0.1161 -0.050794 0.015266 -0.2803 -0.12486 0.23587 0.2339 -0.14023 0.028462 0.56923 -0.1649 -0.036429 0.010051 -0.17107 -0.042608 0.044965 -0.4393 -0.26137 0.30088 -0.060772 -0.45312 -0.19076 -0.20288 0.27694 -0.060888 0.11944 0.62206 -0.19343 0.47849 -0.30113 0.059389 0.074901 0.061068 -0.4662 0.40054 -0.19099 -0.14331 0.018267 -0.18643 0.20709 -0.35598 0.05338 -0.050821 -0.1918 -0.37846 -0.06589\n",
            ". 0.012001 0.20751 -0.12578 -0.59325 0.12525 0.15975 0.13748 -0.33157 -0.13694 1.7893 -0.47094 0.70434 0.26673 -0.089961 -0.18168 0.067226 0.053347 1.5595 -0.2541 0.038413 -0.01409 0.056774 0.023434 0.024042 0.31703 0.19025 -0.37505 0.035603 0.1181 0.012032 -0.037566 -0.5046 -0.049261 0.092351 0.11031 -0.073062 0.33994 0.28239 0.13413 0.070128 -0.022099 -0.28103 0.49607 -0.48693 -0.090964 -0.1538 -0.38011 -0.014228 -0.19392 -0.11068 -0.014088 -0.17906 0.24509 -0.16878 -0.15351 -0.13808 0.02151 0.13699 0.0068061 -0.14915 -0.38169 0.12727 0.44007 0.32678 -0.46117 0.068687 0.34747 0.18827 -0.31837 0.4447 -0.2095 -0.26987 0.48945 0.15388 0.05295 -0.049831 0.11207 0.14881 -0.37003 0.30777 -0.33865 0.045149 -0.18987 0.26634 -0.26401 -0.47556 0.68381 -0.30653 0.24606 0.31611 -0.071098 0.030417 0.088119 0.045025 0.20125 -0.21618 -0.36371 -0.25948 -0.42398 -0.14305 -0.10208 0.21498 -0.21924 -0.17935 0.21546 0.13801 0.24504 -0.2559 0.054815 0.21307 0.2564 -0.25673 0.17961 -0.47638 -0.25181 -0.0091498 -0.054362 -0.21007 0.12597 -0.40795 -0.021164 0.20585 0.18925 -0.0051896 -0.51394 0.28862 -0.077748 -0.27676 0.46567 -0.14225 -0.17879 -0.4357 -0.32481 0.15034 -0.058367 0.49652 0.20472 0.019866 0.13326 0.12823 -1.0177 0.29007 0.28995 0.029994 -0.10763 0.28665 -0.24387 0.22905 -0.26249 -0.069269 -0.17889 0.21936 0.15146 0.04567 -0.050497 0.071482 -0.1027 -0.080705 0.30296 0.031302 0.26613 -0.0060951 0.10313 -0.39987 -0.043945 -0.057625 0.08702 -0.098152 0.22835 -0.005211 0.038075 0.01591 -0.20622 0.021853 0.0040426 -0.043063 -0.002294 -0.26097 -0.25802 -0.28158 -0.23118 -0.010404 -0.30102 -0.4042 0.014653 -0.10445 0.30377 -0.20957 0.3119 0.068272 0.1008 0.010423 0.54011 0.29865 0.12653 0.013761 0.21738 -0.39521 0.066633 0.50327 0.14913 -0.11554 0.010042 0.095698 0.16607 -0.18808 0.055019 0.026715 -0.3164 -0.046583 -0.051591 0.023475 -0.11007 0.085642 0.28394 0.040497 0.071986 0.14157 -0.021199 0.44718 0.20088 -0.12964 -0.067183 0.47614 0.13394 -0.17287 -0.37324 -0.17285 0.02683 -0.1316 0.09116 -0.46487 0.1274 -0.090159 -0.10552 0.068006 -0.13381 0.17056 0.089509 -0.23133 -0.27572 0.061534 -0.051646 0.28377 0.25286 -0.24139 -0.19905 0.12049 -0.1011 0.27392 0.27843 0.26449 -0.18292 -0.048961 0.19198 0.17192 0.33659 -0.20184 -0.34305 -0.24553 -0.15399 0.3945 0.22839 -0.25753 -0.25675 -0.37332 -0.23884 -0.048816 0.78323 0.18851 -0.26477 0.096566 0.062658 -0.30668 -0.43334 0.10006 0.21136 0.039459 -0.11077 0.24421 0.60942 -0.46646 0.086385 -0.39702 -0.23363 0.021307 -0.10778 -0.2281 0.50803 0.11567 0.16165 -0.066737 -0.29556 0.022612 -0.28135 0.0635 0.14019 0.13871 -0.36049 -0.035\n",
            "the 0.27204 -0.06203 -0.1884 0.023225 -0.018158 0.0067192 -0.13877 0.17708 0.17709 2.5882 -0.35179 -0.17312 0.43285 -0.10708 0.15006 -0.19982 -0.19093 1.1871 -0.16207 -0.23538 0.003664 -0.19156 -0.085662 0.039199 -0.066449 -0.04209 -0.19122 0.011679 -0.37138 0.21886 0.0011423 0.4319 -0.14205 0.38059 0.30654 0.020167 -0.18316 -0.0065186 -0.0080549 -0.12063 0.027507 0.29839 -0.22896 -0.22882 0.14671 -0.076301 -0.1268 -0.0066651 -0.052795 0.14258 0.1561 0.05551 -0.16149 0.09629 -0.076533 -0.049971 -0.010195 -0.047641 -0.16679 -0.2394 0.0050141 -0.049175 0.013338 0.41923 -0.10104 0.015111 -0.077706 -0.13471 0.119 0.10802 0.21061 -0.051904 0.18527 0.17856 0.041293 -0.014385 -0.082567 -0.035483 -0.076173 -0.045367 0.089281 0.33672 -0.22099 -0.0067275 0.23983 -0.23147 -0.88592 0.091297 -0.012123 0.013233 -0.25799 -0.02972 0.016754 0.01369 0.32377 0.039546 0.042114 -0.088243 0.30318 0.087747 0.16346 -0.40485 -0.043845 -0.040697 0.20936 -0.77795 0.2997 0.2334 0.14891 -0.39037 -0.053086 0.062922 0.065663 -0.13906 0.094193 0.10344 -0.2797 0.28905 -0.32161 0.020687 0.063254 -0.23257 -0.4352 -0.017049 -0.32744 -0.047064 -0.075149 -0.18788 -0.015017 0.029342 -0.3527 -0.044278 -0.13507 -0.11644 -0.1043 0.1392 0.0039199 0.37603 0.067217 -0.37992 -1.1241 -0.057357 -0.16826 0.03941 0.2604 -0.023866 0.17963 0.13553 0.2139 0.052633 -0.25033 -0.11307 0.22234 0.066597 -0.11161 0.062438 -0.27972 0.19878 -0.36262 -1.0006e-05 -0.17262 0.29166 -0.15723 0.054295 0.06101 -0.39165 0.2766 0.057816 0.39709 0.025229 0.24672 -0.08905 0.15683 -0.2096 -0.22196 0.052394 -0.01136 0.050417 -0.14023 -0.042825 -0.031931 -0.21336 -0.20402 -0.23272 0.07449 0.088202 -0.11063 -0.33526 -0.014028 -0.29429 -0.086911 -0.1321 -0.43616 0.20513 0.0079362 0.48505 0.064237 0.14261 -0.43711 0.12783 -0.13111 0.24673 -0.27496 0.15896 0.43314 0.090286 0.24662 0.066463 -0.20099 0.1101 0.03644 0.17359 -0.15689 -0.086328 -0.17316 0.36975 -0.40317 -0.064814 -0.034166 -0.013773 0.062854 -0.17183 -0.12366 -0.034663 -0.22793 -0.23172 0.239 0.27473 0.15332 0.10661 -0.060982 -0.024805 -0.13478 0.17932 -0.37374 -0.02893 -0.11142 -0.08389 -0.055932 0.068039 -0.10783 0.1465 0.094617 -0.084554 0.067429 -0.3291 0.034082 -0.16747 -0.25997 -0.22917 0.020159 -0.02758 0.16136 -0.18538 0.037665 0.57603 0.20684 0.27941 0.16477 -0.018769 0.12062 0.069648 0.059022 -0.23154 0.24095 -0.3471 0.04854 -0.056502 0.41566 -0.43194 0.4823 -0.051759 -0.27285 -0.25893 0.16555 -0.1831 -0.06734 0.42457 0.010346 0.14237 0.25939 0.17123 -0.13821 -0.066846 0.015981 -0.30193 0.043579 -0.043102 0.35025 -0.19681 -0.4281 0.16899 0.22511 -0.28557 -0.1028 -0.018168 0.11407 0.13015 -0.18317 0.1323\n",
            "and -0.18567 0.066008 -0.25209 -0.11725 0.26513 0.064908 0.12291 -0.093979 0.024321 2.4926 -0.017916 -0.071218 -0.24782 -0.26237 -0.2246 -0.21961 -0.12927 1.0867 -0.66072 -0.031617 -0.057328 0.056903 -0.27939 -0.39825 0.14251 -0.085146 -0.14779 0.055067 -0.0028687 -0.20917 -0.070735 0.22577 -0.15881 -0.10395 0.09711 -0.56251 -0.32929 -0.20853 0.0098711 0.049777 0.0014883 0.15884 0.042771 -0.0026956 -0.02462 -0.19213 -0.22556 0.10838 0.090086 -0.13291 0.32559 -0.17038 -0.1099 -0.23986 -0.024289 0.014656 -0.237 0.084828 -0.35982 -0.076746 0.048909 0.11431 -0.21013 0.24765 -0.017531 -0.14028 0.046191 0.22972 0.1175 0.12724 0.012992 0.4587 0.41085 0.039106 0.15713 -0.18376 0.26834 0.056662 0.16844 -0.053788 -0.091892 0.11193 -0.08681 -0.13324 0.15062 -0.31733 -0.22078 0.25038 0.34131 0.36419 -0.089514 -0.22193 0.24471 0.040091 0.47798 -0.029996 0.0019212 0.063511 -0.20417 -0.26478 0.20649 0.015573 -0.27722 -0.18861 -0.10289 -0.49773 0.14986 -0.010877 0.25085 -0.28117 0.18966 -0.065879 0.094753 -0.15338 -0.055071 -0.36747 0.24993 0.096527 0.23538 0.18405 0.052859 0.22967 0.12582 0.15536 -0.17275 0.33946 -0.10049 0.074948 -0.093575 -0.04049 -0.016922 -0.0058039 -0.18108 0.19537 0.45178 0.10965 0.2337 -0.09905 -0.078633 0.21678 -0.71231 -0.099759 0.33333 -0.1646 -0.091688 0.21056 0.023669 0.028922 0.1199 -0.12512 -0.026037 -0.062217 0.55816 0.0050273 -0.30888 0.038611 0.17568 -0.11163 -0.10815 -0.19444 0.29433 0.14519 -0.042878 0.18534 0.018891 -0.61883 0.13352 0.036007 0.33995 0.22109 -0.079328 0.071319 0.17678 0.16378 -0.23142 -0.1434 -0.098122 -0.019286 0.2356 -0.34013 -0.061007 -0.23208 -0.31152 0.10063 -0.15957 0.20183 -0.016345 -0.12303 0.022667 -0.20986 -0.20127 -0.087883 0.064731 0.10195 -0.1786 0.33056 0.21407 -0.32165 -0.17106 0.19407 -0.38618 -0.2148 -0.052254 0.023175 0.47389 0.18612 0.12711 0.20855 -0.10256 -0.12016 -0.40488 0.029695 -0.027419 -0.0085227 -0.11415 0.081134 -0.17228 0.19142 0.026514 0.043789 -0.12399 0.13354 0.10112 0.081682 -0.15085 0.0075806 -0.18971 0.24669 0.22491 0.35553 -0.3277 -0.21821 0.1402 0.28604 0.055226 -0.086544 0.02111 -0.19236 0.074245 0.076782 0.00081666 0.034097 -0.57719 0.10657 0.28134 -0.11964 -0.68281 -0.32893 -0.24442 -0.025847 0.0091273 0.2025 -0.050959 -0.11042 0.010962 0.076773 0.40048 -0.40739 -0.44773 0.31954 -0.036326 -0.012789 -0.17282 0.1476 0.2356 0.080642 -0.36528 -0.0083443 0.6239 -0.24379 0.019917 -0.28803 -0.010494 0.038412 -0.11718 -0.072462 0.16381 0.38488 -0.029783 0.23444 0.4532 0.14815 -0.027021 -0.073181 -0.1147 -0.0054545 0.47796 0.090912 0.094489 -0.36882 -0.59396 -0.097729 0.20072 0.17055 -0.0047356 -0.039709 0.32498 -0.023452 0.12302 0.3312\n",
            "to 0.31924 0.06316 -0.27858 0.2612 0.079248 -0.21462 -0.10495 0.15495 -0.03353 2.4834 -0.50904 0.08749 0.21426 0.22151 -0.25234 -0.097544 -0.1927 1.3606 -0.11592 -0.10383 0.21929 0.11997 -0.11063 0.14212 -0.16643 0.21815 0.0042086 -0.070012 -0.23532 -0.26518 0.031248 0.16669 -0.089777 0.20059 0.31614 -0.5583 0.075735 0.27635 0.12741 -0.18185 -0.12722 0.024686 -0.077233 -0.48998 0.020355 0.0039164 0.1215 0.089723 -0.078975 0.081443 -0.099087 -0.055621 0.10737 -0.0044042 0.48496 0.11717 -0.017329 0.109 -0.35558 0.051084 0.15714 0.17961 -0.29711 0.033645 -0.025792 -0.013931 -0.23 -0.040306 0.22282 -0.013544 0.011554 0.3911 0.26533 -0.31012 0.40539 -0.042975 0.020811 -0.33033 0.19573 -0.037958 0.10274 -0.0013581 -0.44505 0.077886 0.08511 -0.20285 -0.19481 0.056933 0.53105 0.034154 -0.56996 -0.18469 0.093403 0.28044 -0.23349 0.10938 -0.014288 -0.274 0.034196 -0.098479 0.13268 0.19437 0.13463 -0.099059 0.040324 -0.66272 0.3571 0.15429 0.18598 0.087542 0.080538 -0.25121 0.24155 0.1783 0.036011 -0.027677 0.21161 -0.29107 -0.0083456 0.11317 0.31064 -0.10693 -0.27367 -0.039785 0.039881 0.034462 -0.16518 0.16115 0.060826 0.3075 -0.22398 0.14619 -0.2661 0.49732 -0.13996 -0.24287 0.039469 -0.084495 -0.24315 0.070701 -1.0136 -0.21733 -0.36878 -0.24973 0.17472 -0.011592 0.068561 -0.090411 0.21878 -0.2639 0.11904 0.14285 -0.18707 -0.13474 -0.13232 -0.26553 0.22947 -0.018215 0.0067383 -0.1019 0.10053 -0.1127 -0.13295 0.15951 0.14906 -0.095578 0.26992 0.011057 0.056568 0.021386 0.20215 0.00048589 0.5336 -0.22947 0.29275 0.17378 0.25423 -0.10976 0.058816 0.014616 -0.04306 0.10732 -0.028149 -0.19181 0.1025 -0.063892 0.012737 -0.12913 0.015037 0.26562 -0.017049 -0.060716 -0.094919 0.017775 0.13221 0.1683 -0.19323 -0.17612 0.075506 0.18939 0.12508 -0.1988 -0.16017 -0.21092 0.46933 0.044747 0.098349 0.011637 0.22281 -0.010837 -0.04833 -0.47335 -0.36811 -0.13592 -0.15086 0.25416 0.069531 0.14211 -0.26703 -0.1259 0.12076 -0.26117 0.033024 -0.034398 -0.13968 0.13446 -0.16709 0.15002 -0.13724 0.091226 -0.27718 0.020098 0.26919 0.43016 0.094019 -0.085496 -0.25192 -0.11645 -0.039734 0.0046738 0.54178 -0.16636 0.34546 0.098501 0.47819 -0.38428 -0.3238 -0.14822 -0.47817 0.16704 -0.064505 0.11834 -0.3448 0.096891 0.32309 0.41471 0.19463 -0.20891 -0.12223 -0.058298 -0.20268 0.2948 0.043397 0.10112 0.27177 -0.52124 -0.073794 0.044808 0.41388 0.088782 0.62255 -0.072391 0.090129 0.15428 0.023163 -0.13028 0.061762 0.33803 -0.091581 0.21039 0.05108 0.19184 0.10444 0.2138 -0.35091 -0.23702 0.038399 -0.10031 0.18359 0.025178 -0.12977 0.3713 0.18888 -0.0042738 -0.10645 -0.2581 -0.044629 0.082745 0.097801 0.25045\n",
            "of 0.060216 0.21799 -0.04249 -0.38618 -0.15388 0.034635 0.22243 0.21718 0.0068483 2.4375 -0.27418 0.13572 0.31086 -0.063206 0.00038225 -0.18597 -0.19333 1.4447 -0.38541 -0.28549 0.075627 -0.036799 -0.46068 -0.016835 0.19821 -0.092746 0.18954 -0.00032648 -0.17081 0.50359 0.46256 0.26901 -0.12256 0.24713 0.069305 -0.20777 -0.4456 0.30223 -0.0098344 0.32772 0.11038 0.41271 -0.15854 -0.056983 0.38918 -0.21158 -0.13307 0.40406 0.1749 0.053949 0.10984 -0.18476 -0.054014 0.040112 -0.10175 0.12662 0.069709 -0.24071 -0.20995 -0.051381 0.28219 0.18598 -0.5018 0.27572 -0.18497 -0.18399 0.15696 -0.038444 -0.52238 0.22753 0.048672 -0.078837 0.065448 0.18399 0.40211 -0.12745 -0.12302 0.31072 0.099588 0.036047 -0.25946 0.36128 0.12748 -0.18667 0.16502 -0.3912 -0.67549 0.11291 0.040743 0.034973 -0.04091 -0.039791 -0.40544 -0.015867 0.10239 0.046868 -0.082776 0.015132 -0.14899 -0.25125 0.25244 -0.11851 -0.34127 0.016516 0.30405 -0.541 0.305 0.39065 0.42362 -0.41721 -0.054247 -0.26014 -0.14048 -0.14166 -0.02105 0.050822 -0.078053 0.45922 0.17598 -0.0157 0.09118 0.034263 -0.49995 0.028574 0.12068 0.19781 -0.013025 -0.22418 0.12503 0.14653 -0.23085 0.21987 -0.059321 -0.088169 -0.1252 0.0075112 -0.22421 0.6214 0.2009 -0.02899 -0.65073 0.0053506 -0.12073 0.20988 -0.1684 0.041826 0.054582 0.35247 0.2006 0.031903 -0.053307 -0.44009 0.22495 -0.30616 -0.32855 -0.015779 -0.13913 0.34309 -0.13569 -0.22276 0.14295 0.05501 -0.10616 0.23597 -0.20701 -0.30963 0.13528 -0.16144 0.29108 0.12301 0.2365 -0.26153 0.31022 0.20612 -0.19885 0.10971 -0.0018054 0.14621 0.15177 -0.4468 0.0067433 -0.028784 0.13821 -0.16566 -0.45517 0.016623 0.10703 -0.48399 0.040033 0.049625 -0.26454 -0.1468 0.13651 0.15261 0.067522 0.50405 -0.18848 0.15256 -0.26997 0.055578 0.047077 -0.17848 -0.33567 -0.03148 0.19107 0.18818 0.18778 0.18313 -0.364 -0.0054127 -0.15763 0.16386 -0.084828 -0.19838 -0.40454 0.41031 -0.41393 0.029771 0.10544 -0.11295 -0.068076 -0.22372 -0.19084 -0.080269 -0.38345 0.064712 0.23111 0.21408 0.28038 0.14221 -0.20696 0.015874 -0.14112 0.089859 -0.21533 -0.020105 0.22703 0.083425 -0.2958 0.018036 0.19885 0.17794 0.13688 -0.10302 0.029651 0.051271 -0.14787 -0.41824 0.019828 -0.26385 -0.074654 -0.015718 0.48094 0.12492 -0.11409 0.58127 0.095836 -0.095912 -0.057435 0.13883 0.10307 0.081362 -0.4669 0.50705 0.021685 -0.071623 -0.063827 -0.11154 0.61792 -0.56329 0.023565 0.18041 -0.2578 -0.50956 0.14737 -0.033317 -0.037053 0.24062 0.12641 -0.027091 0.4039 -0.02836 -0.022235 -0.11493 -0.2285 -0.05746 0.2952 -0.21914 -0.13307 -0.23647 -0.42484 0.11606 0.0048131 -0.39629 -0.26823 0.3292 -0.17597 0.11709 -0.16692 -0.094085\n",
            "a 0.043798 0.024779 -0.20937 0.49745 0.36019 -0.37503 -0.052078 -0.60555 0.036744 2.2085 -0.23389 -0.06836 -0.22355 -0.053989 -0.15198 -0.17319 0.053355 1.6485 -0.047991 -0.085311 -0.15712 -0.64425 -0.39819 0.278 0.15364 0.031678 0.055414 0.015939 0.31851 -0.058979 0.038584 0.1077 0.1041 -0.077346 0.37396 -0.21482 0.3832 -0.27737 -0.18352 -0.83838 0.34124 0.58164 0.18543 -0.31028 0.17666 -0.069421 -0.34422 -0.13665 -0.10823 0.23637 -0.32923 0.61348 0.1972 0.087123 0.10785 0.3073 0.13757 0.30809 0.24331 -0.29422 -0.0098214 0.55675 -0.04888 0.099468 0.30543 -0.37597 -0.19525 0.046246 -0.036675 0.34023 0.14905 0.0978 -0.26664 0.056834 -0.043201 -0.23338 0.13111 -0.35742 -0.3607 0.30997 -0.19727 -0.1432 -0.16747 0.00042435 -0.1512 0.067562 -0.38644 0.025349 0.24918 -0.23955 -0.15615 0.49868 0.0082758 -0.1912 -0.14906 0.48757 -0.015281 0.010196 0.37642 -0.01946 -0.27835 0.16355 -0.24127 -0.21405 -0.21562 -0.79697 0.34321 0.093209 0.073977 -0.27147 0.20539 0.15061 0.020734 0.11267 0.028714 0.2967 -0.21267 0.43214 0.12788 0.29249 0.19056 -0.29113 -0.11382 -0.038242 -0.2029 0.18301 -0.16661 -0.27116 0.0012685 0.071704 -0.18583 0.08985 -0.039895 0.39479 0.0053211 -0.00061548 -0.27082 -0.089782 -0.2879 -0.14865 -1.3746 0.16515 0.20598 0.15252 0.034723 -0.38531 -0.094574 -0.19871 0.50239 -0.28702 -0.088727 0.056881 0.13634 0.19034 -0.19353 0.40506 -0.19317 0.22908 0.10055 -0.26895 -0.034727 -0.08401 0.057806 0.011076 -0.043349 -0.26917 -0.19333 0.22181 0.26123 -0.11761 0.10092 -0.15078 0.47153 0.11253 -0.26749 -0.038785 -0.03652 -0.089248 -0.24427 -0.041381 -0.021785 -0.35738 -0.063409 -0.53983 -0.010112 0.00041238 -0.097049 0.42628 -0.21349 -0.41055 -0.2494 -0.033571 -0.4954 0.15557 0.19882 0.10498 -0.24372 0.11429 -0.039279 -0.36258 0.10318 0.129 -0.41785 -0.041607 0.33522 0.073186 0.13362 0.010812 0.052645 0.18801 -0.30185 0.20333 -0.32258 -0.24673 0.21124 0.79132 -0.41539 0.3622 0.099852 -0.035378 -0.0419 -0.13851 -0.063255 0.13635 0.090863 -0.3994 0.099062 0.3221 -0.12256 -0.085906 -0.10218 0.2635 -0.18689 -0.1856 -0.43923 -0.325 -0.1991 0.17831 -0.27283 0.33473 0.082382 0.12825 0.39275 -0.034929 0.16148 -0.026713 0.40129 -0.39503 -0.064823 -0.08982 -0.066592 -0.34537 0.046283 0.36837 -0.024573 0.32213 0.30641 -0.28112 0.0066449 0.087743 -0.03417 0.60373 0.4212 -0.073349 0.26682 -0.1586 0.23765 -0.0062604 0.15236 -0.23409 0.31634 -0.08786 -0.15747 -0.24955 -0.18766 -0.096743 -0.27994 -0.24334 0.32643 0.29906 0.42763 0.22266 -0.17464 -0.019916 -0.31206 -0.34009 -0.14993 -0.28818 0.1475 -0.040503 -0.10347 0.0033634 0.2176 -0.20409 0.092415 0.080421 -0.061246 -0.30099 -0.14584 0.28188\n",
            "in 0.089187 0.25792 0.26282 -0.029365 0.47187 -0.10389 -0.10013 0.08123 0.20883 2.5726 -0.67854 0.036121 0.13085 0.0012462 0.14769 0.26926 0.37144 1.3501 -0.11326 -0.23036 -0.26575 -0.18077 0.092455 -0.16215 0.15003 -0.34547 0.072295 0.40659 0.010021 -0.0079257 -0.11435 0.017008 -0.29789 0.19079 0.37112 -0.26588 0.16212 0.065469 -0.31781 -0.03226 0.081969 0.3445 -0.17362 -0.35745 0.054487 0.39941 0.13699 -0.022066 0.11025 -0.41898 0.1276 -0.095869 -0.17944 -0.17443 0.27302 -0.19464 0.26747 -0.28241 0.1638 -0.11518 0.013196 -0.10616 -0.36093 0.023634 0.13464 0.021652 -0.27094 -0.018737 0.10017 0.36071 -0.093951 0.47634 0.12874 0.0011868 0.1377 -0.14034 -0.1887 -0.16405 -0.15349 0.32347 -0.17616 0.3523 -0.023531 -0.19121 -0.054809 -0.099521 -0.30056 0.36632 -0.21509 0.074123 -0.20267 0.1286 -0.38111 -0.025482 0.45103 0.088633 0.36288 -0.23406 -0.086024 -0.50604 0.034242 0.43998 -0.083023 -0.11969 0.68686 -0.34115 0.21228 0.40039 0.26367 -0.37144 0.16206 -0.42854 0.078658 -0.2905 0.21727 -0.27484 0.35887 0.27055 -0.11326 -0.14848 -0.0050659 -0.076862 0.078621 -0.24922 0.42026 -0.069698 0.071595 0.0071665 0.27473 -0.15664 0.25713 -0.058461 -0.29733 -0.090996 0.5246 0.14889 -0.20883 -0.13004 -0.20022 0.4503 -0.34654 -0.26007 0.35247 -0.34757 0.033738 0.19907 -0.32912 -0.084689 0.65319 0.20954 0.079274 0.1086 0.0026466 -0.12843 -0.22811 0.051501 -0.27429 0.14505 -0.1843 -0.34825 -0.11701 0.34034 0.075848 0.08239 -0.39188 -0.022312 -0.080373 0.14477 0.29701 -0.10523 0.092893 0.029813 -0.11761 0.16308 0.098382 0.46152 -0.162 -0.2456 0.20293 -0.11344 0.057902 -0.19528 -0.20141 -0.22874 -0.014101 0.2637 -0.10028 -0.051896 0.18859 -0.17767 -0.11556 0.121 0.17303 0.11773 0.034837 0.28485 -0.30447 0.061024 -0.26442 -0.081135 -0.044524 -0.036931 -0.15217 0.29175 0.44926 -0.28875 0.33193 -0.01242 -0.18805 -0.19832 -0.19736 0.26893 0.11106 -0.67383 -0.1518 -0.16615 -0.16563 0.0093671 -0.15945 -0.33468 0.22038 -0.16724 -0.1535 -0.61782 -0.17258 0.088928 0.019411 0.18296 0.32967 -0.0024906 -0.09208 0.514 0.0042484 -0.084377 -0.71448 -0.22148 -0.04835 0.043761 -0.29376 -0.22287 0.18001 0.072197 0.46499 0.056466 0.40844 -0.23641 -0.038946 0.087363 -0.21901 -0.3231 -0.19989 -0.3128 -0.067656 -0.22596 0.090926 0.28365 0.31462 0.46082 -0.024871 -0.14605 0.30454 0.17704 -0.011311 0.26807 -0.032461 -0.16644 -0.15313 -0.20426 -0.3082 -0.2459 0.085848 -0.11767 -0.063056 -0.18133 -0.18629 -0.17694 0.29618 0.35987 0.0020102 0.38616 0.36712 -0.055112 -0.34733 -0.072678 -0.051119 -0.29069 0.053598 0.019587 0.16808 -0.27456 -0.097179 -0.054541 0.19229 -0.48128 -0.20304 0.19368 -0.32546 0.14421 -0.169 0.26501\n",
            "\" -0.075242 0.57337 -0.31908 -0.18484 0.88867 -0.27381 0.077588 0.13905 -0.47746 1.4442 -0.56159 0.085829 0.27504 0.1567 0.088067 0.038404 0.13146 0.80903 -0.16476 -0.26437 -0.25213 -0.10082 0.23976 -0.0017618 -0.14791 -0.042768 0.087014 0.4747 -0.0018207 -0.38313 -0.18743 -0.17626 -0.31186 0.11831 0.23195 -0.19336 -0.54827 -0.11649 -0.23389 -0.04854 -0.31656 0.04927 0.0074875 -0.3366 0.39277 -0.25234 -0.24983 0.19855 -0.038369 -0.13467 -0.11403 0.37989 -0.16665 -0.090585 0.22177 0.28434 0.25214 -0.03522 -0.0362 0.092851 -0.1177 -0.1239 0.75229 -0.33378 0.27406 -0.14008 0.12932 -0.081124 -0.23386 0.53654 0.45947 -0.14284 0.080147 0.12924 -0.24142 -0.50243 -0.16263 0.22523 0.528 0.089438 0.010835 -0.15715 -0.49085 -0.30451 -0.06281 -0.033262 -0.34762 -0.61321 0.12422 0.52602 -0.14759 -0.29829 -0.28189 -0.075447 -0.52833 0.20302 -0.36833 -0.095067 0.49005 -0.28781 -0.15406 0.2827 -0.085221 -0.24289 -0.22717 0.71889 -0.32832 0.1002 0.20946 0.59244 -0.41214 0.62329 -0.026238 -0.17192 0.49179 -0.0016847 -0.11607 -0.24131 0.11644 -0.1921 0.03934 -0.084155 -0.0996 0.022975 -0.31975 -0.050044 0.52722 0.2252 0.016595 0.20643 -0.0098633 0.31594 -0.078281 0.12947 -0.062864 0.52739 -0.09028 0.14762 0.043752 0.41388 -1.2517 0.16028 0.029431 0.37164 -0.14389 -0.090422 0.33523 0.65587 0.029611 -0.31848 0.15365 0.080007 -0.30963 -0.082044 0.25966 0.070255 -0.56391 -0.70405 -0.16871 0.025228 -0.31917 0.095542 0.03875 -0.27151 -0.45413 -0.27367 0.15582 -0.23491 0.017226 -0.12817 0.19779 -0.070536 -0.46543 -0.30177 0.126 0.28083 0.27455 -0.097111 -0.097692 -0.49537 -0.34073 -0.22939 -0.14542 0.036665 -0.71022 0.3501 -0.23481 0.73786 -0.1357 -0.15819 -0.25091 0.1612 -0.17855 0.19617 0.13535 0.29201 -0.33659 -0.34374 0.022833 0.72946 -0.14079 -0.23412 -0.23124 0.043603 -0.14916 -0.13349 0.26499 -0.072018 0.26515 -0.11531 -0.049195 -0.3157 0.16259 -0.21417 0.26391 0.19793 -0.14555 0.1195 -0.6928 -0.50451 0.21449 0.49722 -0.15937 0.07369 0.28543 -0.16188 0.043208 0.089521 -0.061378 0.005429 0.23594 -0.16455 0.6693 -0.68702 -0.02823 0.30058 -0.053056 -0.35263 -0.21279 0.2776 -0.067762 0.045954 0.23585 -0.18751 0.13449 -0.10923 -0.010326 -0.3995 -0.44222 0.0080668 0.094805 -0.10209 -0.17181 -0.15658 -0.12305 0.61632 0.050099 0.018423 -0.204 0.090514 0.19061 0.022614 0.28797 0.14858 0.18576 -0.025255 -0.061938 -0.20647 0.43831 -0.16193 0.013259 -0.40052 -0.20146 -0.42659 0.38983 -0.084942 0.13971 0.011352 0.025489 0.25282 -0.10832 -0.081995 0.29953 -0.22407 -0.11489 -0.37855 -0.52748 -0.17067 0.16029 0.29872 -0.035604 -0.022669 0.42531 0.063414 0.36213 -0.2128 -0.22615 0.328 -0.10934 -0.37948\n",
            ": 0.008746 0.33214 -0.29175 -0.15119 -0.41842 -0.23931 -0.23458 -0.055618 -0.09896 0.75175 -0.66615 -0.10734 0.021663 -0.12194 0.022265 0.029731 0.036949 1.3326 -0.10886 -0.22681 -0.28436 0.021524 0.22749 -0.093169 -0.11529 0.51138 0.13868 -0.10885 -0.11482 -0.0074179 0.16234 0.0082633 -0.0023698 -0.39662 0.29591 0.22499 -0.46529 0.40232 0.027284 0.14321 0.034624 0.36936 -0.37351 0.22866 -0.29724 0.28951 -0.44012 0.47265 -0.070029 0.54446 0.30543 0.28181 0.063914 -0.30986 -0.40254 -0.032463 -0.39762 0.45387 0.075187 0.068059 0.12686 0.056289 0.29042 0.2362 0.34559 -0.14253 -0.016066 -0.058892 0.22277 0.31318 -0.37625 -0.044296 -0.017026 0.14938 0.87661 0.30364 -0.57488 -0.075509 -0.14493 0.16592 -0.67818 0.45022 -0.23441 -0.077216 0.32643 -0.1757 -0.0067939 -0.51045 0.56891 0.16143 0.18519 0.037305 -0.4579 -0.12869 0.19132 -0.38693 -0.1352 0.050239 0.36475 -0.061642 -0.181 -0.19424 0.46758 -0.25859 0.00027713 1.8061 -0.031111 -0.253 -0.043878 0.33484 0.21194 -0.16946 -0.012677 0.10138 -0.067128 0.2808 0.16923 -0.30368 -0.36514 0.18905 -0.36382 0.25917 0.18678 -0.054908 -0.068399 -0.083712 0.56264 -0.058912 -0.11257 -0.47151 0.62617 0.16101 0.17465 0.29054 -0.17968 0.17995 -0.28868 0.14772 -0.15869 0.12875 -0.050562 0.12813 0.018328 0.087067 -0.42093 0.26649 0.033762 0.46031 0.091036 -0.23591 0.37056 0.061394 -0.1232 -0.3872 0.31078 -0.40397 0.21185 0.14069 -0.32281 0.052793 -0.34045 -0.75221 -0.063703 0.094202 -0.45663 -0.53987 0.48825 -0.18757 -0.19803 0.40647 -0.24817 0.22975 0.21493 -0.48105 0.20716 0.32023 0.63723 -0.069866 0.3692 -0.15806 0.15572 0.36047 -0.010431 -0.27427 -0.087574 -0.37989 -0.1767 0.13558 0.056266 0.10345 -0.40615 0.11801 -0.32919 0.14333 -0.30102 -0.10898 -0.25298 0.33375 0.27642 0.71642 -0.091013 -0.002913 -0.19669 -0.39123 -0.056526 -0.1143 -0.28571 0.17814 -0.038271 -0.19628 -0.0057383 -0.68218 0.55404 -0.31276 -0.11263 -0.16157 -0.40151 0.40366 -0.21163 0.13927 0.32245 0.65676 0.039262 0.1051 -0.40708 -0.061696 0.30114 0.14276 0.24082 -0.29747 0.047918 0.3043 -0.15456 -0.27875 -0.39602 0.26501 -0.19017 0.054386 0.31772 0.44834 0.18064 -0.27069 0.15007 -0.037164 0.35867 0.25197 -0.42951 -0.080519 0.18769 0.35934 -0.12622 -0.034525 -0.44941 -0.27189 0.1923 0.3202 0.085719 -0.31613 0.12747 0.41687 -0.033986 0.16322 0.093101 0.012885 -0.13576 -0.50731 0.34072 0.01102 0.33894 0.043664 -0.22551 0.067386 -0.0061831 -0.10494 0.059349 0.43297 0.55025 0.30155 -0.1616 0.18268 -0.27236 -0.027163 0.61137 0.0027296 0.13913 0.051779 -0.19778 -0.03439 -0.088886 -0.096511 0.33936 -0.041628 -0.5592 0.22176 -0.41515 0.70059 -0.21371 -0.28677 -0.22663 -0.05087\n"
          ]
        }
      ],
      "source": [
        "#look at the format of the file\n",
        "!head glove.840B.300d.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFe9SkOcYugK"
      },
      "source": [
        "#### Get Glove embeddings\n",
        "In this section we want to populate the `glove` dictionary with a mapping of word to the embedding. Remember: the embedding should be an `np.array` of type `np.float` The glove dictionary should only have words that are present in the train vocabulary.\n",
        "\n",
        "\n",
        "Hint: For getting the word and corresponding embedding from the glove file, remember refer to the above structure of the word to embedding mapping."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "4RNiORwiYTzi",
        "ExecuteTime": {
          "end_time": "2024-02-27T11:38:29.636491Z",
          "start_time": "2024-02-27T11:38:29.629384Z"
        }
      },
      "outputs": [],
      "source": [
        "#takes about 1 minute to read through the whole file and find the words we need.\n",
        "def get_glove_mapping(vocab, file):\n",
        "    \"\"\"\n",
        "    Gets the mapping of words from the vocabulary to pretrained embeddings\n",
        "\n",
        "    INPUT:\n",
        "    vocab       - set of vocabulary words\n",
        "    file        - file with pretrained embeddings\n",
        "\n",
        "    OUTPUT:\n",
        "    glove_map   - mapping of words in the vocabulary to the pretrained embedding\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    glove_map = {}\n",
        "    with open(file,'rb') as fi:\n",
        "        for l in fi:\n",
        "            try:\n",
        "                #### STUDENT CODE HERE ####\n",
        "                ## only include vectors that are found in the vocabulary.\n",
        "                l = l.decode(\"utf-8\")\n",
        "                lst = l.split()\n",
        "                word = lst[0]\n",
        "                if word in vocab:\n",
        "                    numbers = lst[1:]\n",
        "                    numbers = [float(i) for i in numbers]\n",
        "                    vector = np.array(numbers, dtype=np.float32)\n",
        "                    glove_map[word] = vector\n",
        "\n",
        "                #### STUDENT CODE ENDS HERE ####\n",
        "            except:\n",
        "                #some lines have urls, we don't need them.\n",
        "                pass\n",
        "    return glove_map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "kYettRoymWTz",
        "ExecuteTime": {
          "end_time": "2024-02-27T11:39:34.909491Z",
          "start_time": "2024-02-27T11:39:14.132802Z"
        }
      },
      "outputs": [],
      "source": [
        "vocab_set = set(ds.vocab.ordered_words())\n",
        "glove_map = get_glove_mapping(vocab_set,\"/content/glove.840B.300d.txt\")\n",
        "glove_keys = glove_map.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6xDV6QqZfnPP",
        "outputId": "607b7a53-0a81-44c7-d3c8-e534f58b9636"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correct! You earned 2/2 points. You are a star!\n",
            "\n",
            "Your submission has been successfully recorded in the gradebook.\n"
          ]
        }
      ],
      "source": [
        "# PennGrader Grading Cell, do not modify.\n",
        "# This tests you have the right number of keys, and that a few elements are in there.\n",
        "grader.grade(test_case_id = 'test_glove_embedding', answer = list(glove_map.keys()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HoZuTVB9mcZb"
      },
      "source": [
        "### 3.2 Embedding Matrix\n",
        "\n",
        "Fill in the following constants about the system we are building.  These are required for setting up the DAN."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "djhLc9X_Z2E0",
        "ExecuteTime": {
          "end_time": "2024-02-27T11:39:34.911992Z",
          "start_time": "2024-02-27T11:39:34.910167Z"
        }
      },
      "outputs": [],
      "source": [
        "## TODO\n",
        "d_out = 2 # the number of output classes of the model\n",
        "n_embed = 16474 # the total number of word embeddings in the input layer\n",
        "d_embed = 300 # the dimensionality of each word embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g3wlXNeT_KSj",
        "outputId": "8cf37977-e070-4e56-f586-613ac94a423f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correct! You earned 1/1 points. You are a star!\n",
            "\n",
            "Your submission has been successfully recorded in the gradebook.\n"
          ]
        }
      ],
      "source": [
        "# PennGrader Grading Cell, do not modify.\n",
        "# This tests you have the right number of keys, and that a few elements are in there\n",
        "grader.grade(test_case_id = 'test_dimensions', answer = (d_out, n_embed, d_embed))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ct2rwpFxlytq"
      },
      "source": [
        "#### Initializing the embedding matrix\n",
        "\n",
        "Create an embedding_matrix for the parameters to be learned. Fill in the rows of the embedding matrix with the glove embeddings. Use word id as row index.  If you do not find a particular word or you need to randomize the embeddings, initialize the embedding matrix with `np.random.normal`\n",
        "\n",
        "Hint: `ds.vocab.ordered_words()` can give you the mapping of id to words. `glove` has the embeddings you need."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "MMIZjt4HYuJB",
        "ExecuteTime": {
          "end_time": "2024-02-27T11:39:34.915127Z",
          "start_time": "2024-02-27T11:39:34.913554Z"
        }
      },
      "outputs": [],
      "source": [
        "def get_embedding_matrix(n_embed, d_embed, glove_map, randomize_init = False):\n",
        "    \"\"\"\n",
        "    Initialize the embedding matrix\n",
        "\n",
        "    INPUT:\n",
        "    n_embed         - size of the dictionary of embeddings\n",
        "    d_embed         - the size of each embedding vector\n",
        "    glove_map       - the map you created storing all of embeddings you will need from GloVE\n",
        "    randomize_init  - if True, ignore the embeddings from glove_map and intilize all embeddings to random gaussian noise (np.random.normal will be useful).\n",
        "\n",
        "    OUTPUT:\n",
        "    embedding_matrix  - a numpy matrix of mapping from word id to embedding\n",
        "\n",
        "    \"\"\"\n",
        "    #### STUDENT CODE HERE ####\n",
        "    embedding_matrix = np.zeros((n_embed, d_embed), dtype=np.float32)\n",
        "\n",
        "    if not randomize_init:\n",
        "      for index, word in enumerate(ds.vocab.ordered_words()):\n",
        "        if word in glove_map.keys():\n",
        "          embedding_matrix[index] = glove_map[word]\n",
        "        else:\n",
        "          embedding_matrix[index] = [np.float32(np.random.normal()) for _ in range(300)]\n",
        "\n",
        "    else:\n",
        "      for index in range(len(embedding_matrix)):\n",
        "        embedding_matrix[index] = [np.float32(np.random.normal()) for _ in range(300)]\n",
        "    #### STUDENT CODE ENDS HERE ####\n",
        "    return embedding_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "rotHEiIqm2sV",
        "ExecuteTime": {
          "end_time": "2024-02-27T11:39:35.090974Z",
          "start_time": "2024-02-27T11:39:34.917394Z"
        }
      },
      "outputs": [],
      "source": [
        "embedding_matrix = get_embedding_matrix(n_embed, d_embed, glove_map)\n",
        "embedding_data = (embedding_matrix.shape, embedding_matrix[0:155])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OTJCtrTZ_ubK",
        "outputId": "33eb5247-8fe6-4a77-d5e3-02b6ac970051"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correct! You earned 2/2 points. You are a star!\n",
            "\n",
            "Your submission has been successfully recorded in the gradebook.\n"
          ]
        }
      ],
      "source": [
        "# PennGrader Grading Cell, do not modify.\n",
        "# This sets that you have the right values in the embedding matrix.\n",
        "grader.grade(test_case_id = 'test_weight_matrix', answer = embedding_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6rGLI6jm-0g"
      },
      "source": [
        "### 3.3 Embedding Layer\n",
        "The following is initilization code for an embedding module we will include in our DAN.\n",
        "Use the embedding matrix to create the embedding layer by using `nn.Embedding`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "4XGB1pAabQAf",
        "ExecuteTime": {
          "end_time": "2024-02-27T11:39:35.093131Z",
          "start_time": "2024-02-27T11:39:35.091473Z"
        }
      },
      "outputs": [],
      "source": [
        "def create_emb_layer(embedding_matrix, non_trainable=False):\n",
        "    \"\"\"\n",
        "    Create the embedding layer\n",
        "\n",
        "    INPUT:\n",
        "    embedding_matrix  - matrix of mapping from word id to embedding\n",
        "    non_trainable   - Flag for whether the embedding matrix should be trained.\n",
        "                      If it is set to True, don't update the gradients\n",
        "\n",
        "    OUTPUT:\n",
        "    emb_layer       - embedding layer\n",
        "\n",
        "    \"\"\"\n",
        "    #### STUDENT CODE HERE ####\n",
        "    emb_layer = nn.Embedding.from_pretrained(torch.from_numpy(embedding_matrix), freeze=non_trainable)\n",
        "    #### STUDENT CODE ENDS HERE ####\n",
        "\n",
        "    return emb_layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FI5HX8dGNQyR"
      },
      "source": [
        "### 3.4 Defining the Dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2Z4YahqYBBw"
      },
      "source": [
        "For the ease of batch processing, we are defining the following to use the functionality of the Dataloader in Pytorch.\n",
        "\n",
        "You can read more about it in [this tutorial](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "tOubHTzb8CS2",
        "ExecuteTime": {
          "end_time": "2024-02-27T11:39:35.098095Z",
          "start_time": "2024-02-27T11:39:35.096500Z"
        }
      },
      "outputs": [],
      "source": [
        "class SSTpytorchDataset(Dataset):\n",
        "    def __init__(self, sst_ds, word_dropout = 0.3, split='train'):\n",
        "        super(SSTpytorchDataset, self).__init__()\n",
        "        assert split in ['train', 'test', 'dev'], \"Error!\"\n",
        "        self.ds = sst_ds\n",
        "        self.split = split\n",
        "        self.word_dropout = word_dropout\n",
        "        self.data_x, self.data_ns, self.data_y = self.ds.as_padded_array(split,is_root =is_root)\n",
        "        self.mask = np.zeros_like(self.data_x)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data_x.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "        y = 2\n",
        "        if self.split != 'test':\n",
        "            y = self.data_y[idx]\n",
        "\n",
        "        mask = np.zeros(len(self.data_x[idx]))\n",
        "        sentl = self.data_ns[idx]\n",
        "        total_dropped = 0\n",
        "        for j in range(0,sentl):\n",
        "            mask[j] = 1\n",
        "            if self.split == 'train':\n",
        "                rv = random.random()\n",
        "                if rv  < self.word_dropout:\n",
        "                    mask[j] = 0\n",
        "                    total_dropped+=1\n",
        "        if total_dropped >= sentl:\n",
        "            mask[0] = 1\n",
        "        for i in range(sentl,len(self.data_x[idx])):\n",
        "            mask[i] = 0\n",
        "        self.mask[idx] = mask\n",
        "        return self.data_x[idx], self.data_ns[idx], self.mask[idx], sentl, y\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKlnbGShNCDV"
      },
      "source": [
        "### 3.5 Defining DAN Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QnSMLJcrsYC0"
      },
      "source": [
        "#### Masking in Deep Averaging Networks\n",
        "\n",
        "A lot of NLP applications use masking and deal with variable sequence lengths. We will need to handle this issue in our DAN. The dataloader provides enough information to do this in the form of a mask indicating which elements of the batch correspond to real tokens and pad tokens.\n",
        "\n",
        "You have to perform the following steps:\n",
        "\n",
        "1. Change the view of the mask so it extends to the embeddings size\n",
        "2. Calculate the number of words in the sequence (`den`)\n",
        "3. Eliminate the tokens that aren't in the mask, and sum the rest `num`\n",
        "4. return `x = num/den`\n",
        "\n",
        "Note: You can look at [expand](https://pytorch.org/docs/stable/generated/torch.Tensor.expand.html) in pytorch.\n",
        "\n",
        "Implement this logic in the masked mean function.\n",
        "\n",
        "#### Feedforward network\n",
        "\n",
        "You will also need define the feedforward layers required for the final output of the classifier. We suggest you start simple, and first implement a linear classifier over the average of words embeddings. Then, introduce a feedforward neural network that operates over the average embeddings. Remember, you must include non-linearities in your feed foward network. You will need to experiment with different depths and different non-linearities for your report.\n",
        "\n",
        "Fill in the sections marked with following cells that handle masking and the network definition."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0aq_l_f4xiv"
      },
      "source": [
        "#### Defining the architecture for Deep Averaging Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9UIdMAHwXMoa"
      },
      "source": [
        "### 3.6 Training the Deep Averaging Network\n",
        "\n",
        "The following section implements a training loop for you. Use it to train and validate your DAN. Experiment with enough hyper-parameters to pass the PennGrader test below. You should be able to get over 85 accuracy without overfitting the development set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "iMPfgoTym0Yj",
        "ExecuteTime": {
          "end_time": "2024-02-27T11:42:41.419338Z",
          "start_time": "2024-02-27T11:42:41.407398Z"
        }
      },
      "outputs": [],
      "source": [
        "def train(model, lr = .005, drop_out = 0, word_dropout = .3, batch_size = 16, weight_decay = 1e-5, model_type= \"DAN\"):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    trainset = SSTpytorchDataset(ds, word_dropout, 'train')\n",
        "    testset = SSTpytorchDataset(ds, word_dropout, 'test')\n",
        "    devset = SSTpytorchDataset(ds, word_dropout, 'dev')\n",
        "\n",
        "    train_iter = DataLoader(trainset, batch_size, shuffle=True, num_workers=0)\n",
        "    test_iter = DataLoader(testset, batch_size, shuffle=False, num_workers=0)\n",
        "    dev_iter = DataLoader(devset, batch_size, shuffle=False, num_workers=0)\n",
        "\n",
        "    model = model\n",
        "    model.to(device)\n",
        "\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay = weight_decay)\n",
        "\n",
        "\n",
        "    acc, val_loss = evaluate(dev_iter, model, device, model_type)\n",
        "    best_acc = acc\n",
        "\n",
        "    print(\n",
        "        'epoch |   %        |  loss  |  avg   |val loss|   acc   |  best  | time | save |')\n",
        "    print(\n",
        "        'val   |            |        |        | {:.4f} | {:.4f} | {:.4f} |      |      |'.format(\n",
        "            val_loss, acc, best_acc))\n",
        "\n",
        "    iterations = 0\n",
        "    last_val_iter = 0\n",
        "    train_loss = 0\n",
        "    start = time.time()\n",
        "    _save_ckp = ''\n",
        "    for epoch in range(epochs):\n",
        "        # train_iter.init_epoch()\n",
        "        n_correct, n_total, train_loss = 0, 0, 0\n",
        "        last_val_iter = 0\n",
        "        for batch_idx, batch in enumerate(train_iter):\n",
        "            # switch model to training mode, clear gradient accumulators\n",
        "            model.train()\n",
        "            optimizer.zero_grad()\n",
        "            iterations += 1\n",
        "\n",
        "            data, ns, mask, lens, label = batch\n",
        "            data = data.to(device)\n",
        "            label = label.to(device).long()\n",
        "            mask = mask.to(device).long()\n",
        "            if model_type == \"LSTM\":\n",
        "                answer = model(data, mask, lens)\n",
        "            else:\n",
        "                answer = model(data, mask)\n",
        "\n",
        "            loss = criterion(answer, label)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            print('\\r {:4d} | {:4d}/{} | {:.4f} | {:.4f} |'.format(\n",
        "                epoch, batch_size * (batch_idx + 1), len(trainset), loss.item(),\n",
        "                       train_loss / (iterations - last_val_iter)), end='')\n",
        "\n",
        "            if iterations > 0 and iterations % dev_every == 0:\n",
        "                acc, val_loss= evaluate(dev_iter, model, device, model_type)\n",
        "                if acc > best_acc:\n",
        "                    best_acc = acc\n",
        "                    torch.save(model.state_dict(), save_path)\n",
        "                    _save_ckp = '*'\n",
        "\n",
        "                print(\n",
        "                    ' {:.4f} | {:.4f} | {:.4f} | {:.2f} | {:4s} |'.format(\n",
        "                        val_loss, acc, best_acc, (time.time() - start) / 60,\n",
        "                        _save_ckp))\n",
        "\n",
        "                train_loss = 0\n",
        "                last_val_iter = iterations\n",
        "    model.load_state_dict(torch.load(save_path)) #this will be the best model\n",
        "    test_y_pred = evaluate(test_iter, model, device, model_type, \"test\")\n",
        "    print(\"\\nValidation Accuracy : \", evaluate(dev_iter,model, device, model_type))\n",
        "    return best_acc, test_y_pred\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "qKen8d8knAj8",
        "ExecuteTime": {
          "end_time": "2024-02-27T11:42:41.852577Z",
          "start_time": "2024-02-27T11:42:41.850949Z"
        }
      },
      "outputs": [],
      "source": [
        "def evaluate(loader, model, device, model_type = \"DAN\", split = \"dev\"):\n",
        "    model.eval()\n",
        "    n_correct, n = 0, 0\n",
        "    losses = []\n",
        "    y_pred = []\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, batch in enumerate(loader):\n",
        "            data, ns, mask, lens, label = batch\n",
        "            data = data.to(device)\n",
        "            label = label.to(device).long()\n",
        "            mask = mask.to(device).long()\n",
        "            if model_type == \"LSTM\":\n",
        "                answer = model(data, mask, lens)\n",
        "            else:\n",
        "                answer = model(data, mask)\n",
        "            if split != \"test\":\n",
        "                n_correct += (torch.max(answer, 1)[1].view(label.size()) == label).sum().item()\n",
        "                n += answer.shape[0]\n",
        "                loss = criterion(answer, label)\n",
        "                losses.append(loss.data.cpu().numpy())\n",
        "            else:\n",
        "                y_pred.extend(torch.max(answer, 1)[1].view(label.size()).tolist())\n",
        "    if split != \"test\":\n",
        "        acc = 100. * n_correct / n\n",
        "        loss = np.mean(losses)\n",
        "        return acc, loss\n",
        "    else:\n",
        "        return y_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "HvFPHJzcmwt8",
        "ExecuteTime": {
          "end_time": "2024-02-27T11:42:45.067102Z",
          "start_time": "2024-02-27T11:42:45.059542Z"
        }
      },
      "outputs": [],
      "source": [
        "import random as random\n",
        "\n",
        "class DAN(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                 n_embed=20000,\n",
        "                 d_embed=300,\n",
        "                 d_hidden=100,\n",
        "                 d_out=2,\n",
        "                 layer_dropout = 0,\n",
        "                 embeddings=None,\n",
        "                 depth = 2):\n",
        "        super(DAN, self).__init__()\n",
        "\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.embed = create_emb_layer(embedding_matrix, False)\n",
        "        #### STUDENT CODE HERE ####\n",
        "\n",
        "        ### We need to create the layers to construct the output of the classifier.\n",
        "        ### One possible implementation uses the \"nn.Seqential\" module, which chains together multiple layers.\n",
        "        # self.fc_out = nn.Linear(d_hidden, d_out)\n",
        "        self.feedforward_layers = nn.Sequential(\n",
        "            nn.Linear(d_hidden, d_hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(d_hidden, d_out),\n",
        "        )\n",
        "\n",
        "        #### STUDENT CODE ENDS HERE ####\n",
        "\n",
        "    def masked_mean(self, v, mask):\n",
        "        \"\"\"\n",
        "        Create the masked mean\n",
        "\n",
        "        INPUT:\n",
        "        v       - input\n",
        "        mask    - mask that has 0 and 1 for all th tokens in the input\n",
        "                  0 corresponds to dropping the token and 1 corresponds to keeping the token\n",
        "\n",
        "        OUTPUT:\n",
        "        x       - average\n",
        "\n",
        "        \"\"\"\n",
        "        (batch, max_sent, d_embed) = v.size()\n",
        "        mask.requires_grad = False\n",
        "\n",
        "        #### STUDENT CODE HERE ####\n",
        "\n",
        "        #change the view of the mask so it extends to the embeddings size\n",
        "        #the number surviving words\n",
        "        #eliminate the tokens that aren't in the mask, and sum\n",
        "        #average\n",
        "\n",
        "        new_mask = torch.reshape(mask, (-1, 40, 1))\n",
        "        new_mask = new_mask.expand(-1, 40, 300)\n",
        "\n",
        "        product = v * new_mask\n",
        "        numerator = torch.sum(product, dim=1)\n",
        "\n",
        "        denominator = torch.count_nonzero(mask, dim=1)\n",
        "        denominator = torch.reshape(denominator, (-1, 1))\n",
        "        denominator = denominator.expand(-1, 300)\n",
        "\n",
        "        x = numerator / denominator\n",
        "\n",
        "        #### STUDENT CODE ENDS HERE ####\n",
        "        return x\n",
        "\n",
        "    def forward(self, text, mask):\n",
        "        x = text\n",
        "        x = self.embed(x)\n",
        "        x = self.masked_mean(x, mask)\n",
        "        x = self.feedforward_layers(x)\n",
        "        m = nn.Softmax()\n",
        "        output = m(x)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnR4jGf5XIzF"
      },
      "source": [
        "The following code will train the averaging network on the training data after which it will return the accuracy achieved on the validation set as well as the predictions for the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v7jSq4uunGEx",
        "outputId": "f6a158c5-6f9f-4075-b616-772f8b12ea26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch |   %        |  loss  |  avg   |val loss|   acc   |  best  | time | save |\n",
            "val   |            |        |        | 0.6936 | 48.0064 | 48.0064 |      |      |\n",
            "    0 | 12800/98794 | 0.6207 | 0.6546 | 0.6139 | 70.2633 | 70.2633 | 0.03 | *    |\n",
            "    0 | 25600/98794 | 0.5701 | 0.5733 | 0.5427 | 80.5813 | 80.5813 | 0.06 | *    |\n",
            "    0 | 38400/98794 | 0.5146 | 0.5235 | 0.5065 | 83.6631 | 83.6631 | 0.08 | *    |\n",
            "    0 | 51200/98794 | 0.4990 | 0.5029 | 0.4884 | 84.4925 | 84.4925 | 0.11 | *    |\n",
            "    0 | 64000/98794 | 0.4811 | 0.4876 | 0.4771 | 85.2458 | 85.2458 | 0.14 | *    |\n",
            "    0 | 76800/98794 | 0.5021 | 0.4833 | 0.4694 | 85.6719 | 85.6719 | 0.16 | *    |\n",
            "    0 | 89600/98794 | 0.4724 | 0.4756 | 0.4643 | 85.7252 | 85.7252 | 0.19 | *    |\n",
            "    1 | 3584/98794 | 0.4785 | 0.0166 | 0.4600 | 86.0143 | 86.0143 | 0.22 | *    |\n",
            "    1 | 16384/98794 | 0.4639 | 0.4631 | 0.4561 | 86.1893 | 86.1893 | 0.25 | *    |\n",
            "    1 | 29184/98794 | 0.4681 | 0.4599 | 0.4529 | 85.9610 | 86.1893 | 0.28 | *    |\n",
            "    1 | 41984/98794 | 0.4758 | 0.4558 | 0.4511 | 86.2654 | 86.2654 | 0.31 | *    |\n",
            "    1 | 54784/98794 | 0.4613 | 0.4561 | 0.4487 | 86.3567 | 86.3567 | 0.34 | *    |\n",
            "    1 | 67584/98794 | 0.4452 | 0.4517 | 0.4471 | 86.5469 | 86.5469 | 0.36 | *    |\n",
            "    1 | 80384/98794 | 0.4672 | 0.4509 | 0.4453 | 86.6154 | 86.6154 | 0.43 | *    |\n",
            "    1 | 93184/98794 | 0.4497 | 0.4489 | 0.4439 | 86.6459 | 86.6459 | 0.48 | *    |\n",
            "    2 | 7168/98794 | 0.4564 | 0.0158 | 0.4427 | 86.6611 | 86.6611 | 0.52 | *    |\n",
            "    2 | 19968/98794 | 0.4562 | 0.4406 | 0.4437 | 86.6383 | 86.6611 | 0.55 | *    |\n",
            "    2 | 32768/98794 | 0.4625 | 0.4389 | 0.4406 | 87.1633 | 87.1633 | 0.58 | *    |\n",
            "    2 | 45568/98794 | 0.4240 | 0.4379 | 0.4401 | 87.1405 | 87.1633 | 0.60 | *    |\n",
            "    2 | 58368/98794 | 0.4134 | 0.4418 | 0.4388 | 86.9883 | 87.1633 | 0.63 | *    |\n",
            "    2 | 71168/98794 | 0.4685 | 0.4383 | 0.4386 | 86.8361 | 87.1633 | 0.66 | *    |\n",
            "    2 | 83968/98794 | 0.4465 | 0.4389 | 0.4386 | 87.1937 | 87.1937 | 0.68 | *    |\n",
            "    2 | 96768/98794 | 0.4230 | 0.4369 | 0.4381 | 86.9046 | 87.1937 | 0.71 | *    |\n",
            "    2 | 98816/98794 | 0.4396 | 0.4354 |\n",
            "Validation Accuracy :  (87.19373002587125, 0.43855178)\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(1234)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "lr = 0.0001\n",
        "drop_out = 0\n",
        "word_dropout = 0.25\n",
        "weight_decay = 1e-5\n",
        "batch_size = 128\n",
        "dev_every = 100\n",
        "epochs = 3\n",
        "save_path = \"best_model\"\n",
        "model = DAN(n_embed=n_embed, d_embed=d_embed, d_hidden=300, d_out=d_out, layer_dropout=drop_out)\n",
        "dev_value, test_y_pred = train(model, lr, drop_out, word_dropout, batch_size, weight_decay, \"DAN\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qE3gqpSbPBJ7"
      },
      "source": [
        "After you have trained your DAN, evaluate it on our hidden test set. Your score is a linear function between 83 and 85 on the test set. Experiment with different configurations on the development set until you get at least 85 on the development set. Then verify it generalizes.\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ig9gYTXVl5vM"
      },
      "source": [
        "### 3.7 Self Exploration\n",
        "\n",
        "This is where you experiment with different configurations of training (learning rate and batch size), and the network (dropout rates, depth, hidden sizes) and report which worked best. Illustrate your expermiments with in the form of a table (learning rate vs dev accuracy and batch size vs dev accuracy). When doing this validation, it is not required to train to convergence.\n",
        "You will need to report your results in the report."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DiZyHsyLB8pV",
        "outputId": "727bdc9a-d5d6-4361-ac2a-0eee04ea3a95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correct! You earned 33/33 points. You are a star!\n",
            "\n",
            "Your submission has been successfully recorded in the gradebook.\n"
          ]
        }
      ],
      "source": [
        "#Do not modify this cell. This is the grading cell.\n",
        "grader.grade(test_case_id = 'test_dan_predictions', answer = test_y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AAflZLIpNybW"
      },
      "source": [
        "## 4. LSTM Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQ-eAzulN6CG"
      },
      "source": [
        "\n",
        "\n",
        "![LSTM](https://miro.medium.com/max/1400/1*-kBdBYzR7lpimgb3AIRkOw.png)\n",
        "\n",
        "LSTM stands for Long Short-Term Memory Network and it is a type of Recurrent Neural Network (RNN). As you recall from the lecture, Simple RNNs can struggle with exploding or vanishing gradients and LSTMs try to improve this issue. For the next part of the coding exercise, you will implement an LSTM to solve the classification problem.\n",
        "\n",
        "There are multiple ways to get the classification output from the hidden states of an RNN. For example, you may take the last hidden output, or you may aggregate the hidden states, use max or mean operations. Include a discussion of which method you have used in your report.\n",
        "\n",
        "**Hint:** You might want to think about how to handle different length sentences. You can use [pack_padded_sequence](https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pack_padded_sequence.html) and [pad_packed_sequence](https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pad_packed_sequence.html) to achieve this.\n",
        "\n",
        "**Note:** Bi-directional LSTMs require a concatnation of the output. Consider the PyTorch documentation carefully https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html?highlight=lstm#torch.nn.LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YLr43Dam1CB"
      },
      "source": [
        "### 4.1 Defining the architecture for LSTM Networks\n",
        "\n",
        "This is where you define the architecture for the LSTM Network. Ensure that you also handle the cases where you have to define a Bi-directional LSTM network. We will be reusing the data loader and training code from above, so all you need to do is complete the following class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "1q7oqyrCP8fK",
        "ExecuteTime": {
          "end_time": "2024-02-27T11:43:11.468672Z",
          "start_time": "2024-02-27T11:43:11.457105Z"
        }
      },
      "outputs": [],
      "source": [
        "import random as random\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "from torch.nn import LSTM, GRU\n",
        "\n",
        "class LSTM_Classifier(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                 n_embed=20000,\n",
        "                 d_embed=300,\n",
        "                 d_hidden=100,\n",
        "                 d_out=2,\n",
        "                 embeddings=None,\n",
        "                 nl = 20,\n",
        "                 bidirectional = False,\n",
        "                 feature = 'hn',\n",
        "                 method = 'all'\n",
        "                 ):\n",
        "        super(LSTM_Classifier, self).__init__()\n",
        "\n",
        "        self.d_hidden = d_hidden\n",
        "        self.bidrectional = bidirectional\n",
        "        self.feature = feature\n",
        "        self.method = method\n",
        "        self.num_layers = nl\n",
        "\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.embed = create_emb_layer(embedding_matrix,False)\n",
        "        #### STUDENT CODE STARTS HERE ####\n",
        "        #Define your rnn based archecture here\n",
        "\n",
        "        self.rnn = nn.LSTM(input_size=d_embed, hidden_size=d_hidden,\n",
        "                           num_layers=self.num_layers, bias=True,\n",
        "                           batch_first=True, dropout=0.3,\n",
        "                           bidirectional=self.bidrectional,\n",
        "                           device=self.device)\n",
        "\n",
        "        ## USE THIS WHEN USING THE Hn FEATURE EXTRACTION\n",
        "        if self.feature == 'hn':\n",
        "            self.feedforward_layers = nn.Sequential(\n",
        "                nn.Linear(self.d_hidden * self.num_layers * (2 if self.bidrectional else 1), self.d_hidden * self.num_layers * (2 if self.bidrectional else 1)),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(self.d_hidden * self.num_layers * (2 if self.bidrectional else 1), self.d_hidden * self.num_layers * (2 if self.bidrectional else 1)),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(self.d_hidden * self.num_layers * (2 if self.bidrectional else 1), self.d_hidden * self.num_layers * (2 if self.bidrectional else 1)),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(self.d_hidden * self.num_layers * (2 if self.bidrectional else 1), self.d_hidden * self.num_layers * (2 if self.bidrectional else 1)),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(self.d_hidden * self.num_layers * (2 if self.bidrectional else 1), d_out))\n",
        "\n",
        "        ## USE THIS WHEN USING THE DEEPEST Ht FEATURE EXTRACTION'\n",
        "        elif self.feature == 'ht':\n",
        "            self.feedforward_layers = nn.Sequential(\n",
        "                nn.Linear(self.d_hidden * (2 if self.bidrectional else 1), self.d_hidden * (2 if self.bidrectional else 1)),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(self.d_hidden * (2 if self.bidrectional else 1), self.d_hidden * (2 if self.bidrectional else 1)),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(self.d_hidden * (2 if self.bidrectional else 1), self.d_hidden * (2 if self.bidrectional else 1)),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(self.d_hidden * (2 if self.bidrectional else 1), self.d_hidden * (2 if self.bidrectional else 1)),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(self.d_hidden * (2 if self.bidrectional else 1), d_out))\n",
        "\n",
        "        #### STUDENT CODE ENDS HERE ####\n",
        "\n",
        "    def forward(self, text, mask, seq_lengths):\n",
        "\n",
        "        batch_size = text.size()[0]\n",
        "\n",
        "        #### STUDENT CODE STARTS HERE ####\n",
        "        #compute the forward pass,\n",
        "        #take care to extract the appropriate parts of the rnn computation based on the length of the sentences\n",
        "\n",
        "        ## USING ALL HIDDEN LAYERS\n",
        "        if self.method == 'all':\n",
        "            x = text\n",
        "            x_cpu = x.cpu()\n",
        "            lengths = np.count_nonzero(x_cpu, axis=1)\n",
        "            x = self.embed(x)\n",
        "            packed = torch.nn.utils.rnn.pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n",
        "            packed_output, (hn, cn) = self.rnn(packed)\n",
        "            hn = torch.permute(hn, (1, 2, 0))\n",
        "            hn = torch.flatten(hn, start_dim=1)\n",
        "            output = self.feedforward_layers(hn)\n",
        "            m = nn.Softmax()\n",
        "            output = m(output)\n",
        "\n",
        "        ### USING LAST VALUE OF HIDDEN LAYERS\n",
        "        elif self.method == 'last':\n",
        "            x = text\n",
        "            x_cpu = x.cpu()\n",
        "            lengths = np.count_nonzero(x_cpu, axis=1)\n",
        "            x = self.embed(x)\n",
        "            packed = torch.nn.utils.rnn.pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n",
        "            packed_output, (hn, cn) = self.rnn(packed)\n",
        "            hn = hn[-1]\n",
        "            output = self.feedforward_layers(hn)\n",
        "            m = nn.Softmax()\n",
        "            output = m(output)\n",
        "\n",
        "        ### USING MAX OF HIDDEN LAYER\n",
        "        elif self.method == 'max':\n",
        "            x = text\n",
        "            x_cpu = x.cpu()\n",
        "            lengths = np.count_nonzero(x_cpu, axis=1)\n",
        "            x = self.embed(x)\n",
        "            packed = torch.nn.utils.rnn.pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n",
        "            packed_output, (hn, cn) = self.rnn(packed)\n",
        "            seq_unpacked, lens_unpacked = torch.nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)\n",
        "            max_seq_unpacked = torch.max(seq_unpacked, 1, True)\n",
        "            max_seq_unpacked = max_seq_unpacked.values\n",
        "            flat_max_seq_unpacked = torch.flatten(max_seq_unpacked, start_dim=1)\n",
        "            output = self.feedforward_layers(flat_max_seq_unpacked)\n",
        "            m = nn.Softmax()\n",
        "            output = m(output)\n",
        "\n",
        "        ### USING MEAN OF HIDDEN LAYER\n",
        "        elif self.method == 'mean':\n",
        "            x = text\n",
        "            x_cpu = x.cpu()\n",
        "            lengths = np.count_nonzero(x_cpu, axis=1)\n",
        "            x = self.embed(x)\n",
        "            packed = torch.nn.utils.rnn.pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n",
        "            packed_output, (hn, cn) = self.rnn(packed)\n",
        "            seq_unpacked, lens_unpacked = torch.nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)\n",
        "            mean_seq_unpacked = torch.mean(seq_unpacked, 1, True)\n",
        "            print(mean_seq_unpacked.shape)\n",
        "            flat_mean_seq_unpacked = torch.flatten(mean_seq_unpacked, start_dim=1)\n",
        "            print(flat_mean_seq_unpacked.shape)\n",
        "            output = self.feedforward_layers(flat_mean_seq_unpacked)\n",
        "            m = nn.Softmax()\n",
        "            output = m(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "        #### STUDENT CODE ENDS HERE ####\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmdzEvd2Nc2M"
      },
      "source": [
        "### 4.2 Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x7FkX-MDokS7",
        "outputId": "2353f830-328a-473b-9c67-34e3d1d9d427",
        "ExecuteTime": {
          "end_time": "2024-02-27T11:45:01.461328Z",
          "start_time": "2024-02-27T11:43:13.113919Z"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch |   %        |  loss  |  avg   |val loss|   acc   |  best  | time | save |\n",
            "val   |            |        |        | 0.6930 | 56.7570 | 56.7570 |      |      |\n",
            "    0 | 25600/98794 | 0.4320 | 0.5424 | 0.4713 | 83.6783 | 83.6783 | 0.47 | *    |\n",
            "    0 | 51200/98794 | 0.4395 | 0.4620 | 0.4509 | 85.4741 | 85.4741 | 0.93 | *    |\n",
            "    0 | 76800/98794 | 0.4213 | 0.4423 | 0.4487 | 85.4284 | 85.4741 | 1.34 | *    |\n",
            "    1 | 3584/98794 | 0.4241 | 0.0150 | 0.4410 | 86.2350 | 86.2350 | 1.79 | *    |\n",
            "    1 | 29184/98794 | 0.4138 | 0.4274 | 0.4382 | 86.8057 | 86.8057 | 2.25 | *    |\n",
            "    1 | 54784/98794 | 0.3947 | 0.4243 | 0.4377 | 86.5469 | 86.8057 | 2.66 | *    |\n",
            "    1 | 80384/98794 | 0.4116 | 0.4222 | 0.4386 | 87.0111 | 87.0111 | 3.11 | *    |\n",
            "    2 | 7168/98794 | 0.4276 | 0.0144 | 0.4421 | 86.9198 | 87.0111 | 3.52 | *    |\n",
            "    2 | 32768/98794 | 0.4149 | 0.4142 | 0.4420 | 86.2426 | 87.0111 | 3.93 | *    |\n",
            "    2 | 58368/98794 | 0.4049 | 0.4058 | 0.4383 | 87.0872 | 87.0872 | 4.46 | *    |\n",
            "    2 | 83968/98794 | 0.4124 | 0.4021 | 0.4336 | 87.3383 | 87.3383 | 4.91 | *    |\n",
            "    3 | 10752/98794 | 0.4028 | 0.0140 | 0.4345 | 87.1024 | 87.3383 | 5.32 | *    |\n",
            "    3 | 36352/98794 | 0.4200 | 0.3989 | 0.4330 | 87.4601 | 87.4601 | 5.85 | *    |\n",
            "    3 | 61952/98794 | 0.3743 | 0.3972 | 0.4337 | 87.6275 | 87.6275 | 6.31 | *    |\n",
            "    3 | 87552/98794 | 0.4066 | 0.3983 | 0.4380 | 86.7904 | 87.6275 | 6.72 | *    |\n",
            "    4 | 14336/98794 | 0.3709 | 0.0137 | 0.4318 | 87.4677 | 87.6275 | 7.13 | *    |\n",
            "    4 | 39936/98794 | 0.3946 | 0.3909 | 0.4302 | 87.8633 | 87.8633 | 7.64 | *    |\n",
            "    4 | 63744/98794 | 0.3891 | 0.3867 |"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(1234)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "batch_size = 256\n",
        "epochs = 5\n",
        "dev_every = 100\n",
        "lr = 0.0001\n",
        "save_path = \"best_model\"\n",
        "drop_out = 0.3\n",
        "word_dropout = 0.25\n",
        "weight_decay = 0.001\n",
        "\n",
        "model = LSTM_Classifier(n_embed=n_embed, d_embed=d_embed, d_hidden=300, d_out=d_out, feature='hn', method='all')\n",
        "dev_value, test_y_pred = train(model, lr, drop_out, word_dropout, batch_size, weight_decay, \"LSTM\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7IF9SVAdear"
      },
      "source": [
        "### 4.3 Self Exploration\n",
        "\n",
        "Experiment with different types of RNN networks and see which one works the best for the task (Bidirectional LSTM, GRU, Simple RNN). You have to report the difference in performance between these in your report. You should also tune the hypeparamters (learning rate and batch size, hidden dimension size) and report which worked best. Illustrate your expermiments with in the form of a table (learning rate vs dev accuracy and batch size vs dev accuracy).\n",
        "\n",
        "After you have trained your RNN, evaluate it on our hidden test set. Your score is a linear function between 84 and 88 on the test set. Experiment with different configurations on the development set until you get at least 86 on the development set. Then verify it generalizes.\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dDMZ1ostVPbm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfe38890-2ef2-4e77-e9dc-d8eca6565206"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correct! You earned 35/35 points. You are a star!\n",
            "\n",
            "Your submission has been successfully recorded in the gradebook.\n"
          ]
        }
      ],
      "source": [
        "#Do not modify this cell. This is the grading cell.\n",
        "grader.grade(test_case_id = 'test_rnn_predictions', answer = test_y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P6ouNHOd-Sbg"
      },
      "outputs": [],
      "source": [
        "import random as random\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "from torch.nn import LSTM, GRU\n",
        "\n",
        "class GRU_Classifier(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                 n_embed=20000,\n",
        "                 d_embed=300,\n",
        "                 d_hidden=100,\n",
        "                 d_out=2,\n",
        "                 embeddings=None,\n",
        "                 nl = 20,\n",
        "                 bidirectional = False\n",
        "                 ):\n",
        "        super(GRU_Classifier, self).__init__()\n",
        "\n",
        "        self.d_hidden = d_hidden\n",
        "        self.bidrectional = bidirectional\n",
        "        self.num_layers = nl\n",
        "\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.embed = create_emb_layer(embedding_matrix,False)\n",
        "        #### STUDENT CODE STARTS HERE ####\n",
        "        #Define your rnn based archecture here\n",
        "\n",
        "        self.rnn = nn.GRU(input_size=d_embed, hidden_size=d_hidden,\n",
        "                           num_layers=self.num_layers, bias=True,\n",
        "                           batch_first=True, dropout=0.3,\n",
        "                           bidirectional=self.bidrectional,\n",
        "                           device=self.device)\n",
        "\n",
        "        self.feedforward_layers = nn.Sequential(\n",
        "            nn.Linear(d_hidden * self.num_layers * (2 if self.bidrectional else 1), d_hidden * self.num_layers * (2 if self.bidrectional else 1)),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(d_hidden * self.num_layers * (2 if self.bidrectional else 1), d_hidden * self.num_layers * (2 if self.bidrectional else 1)),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(d_hidden * self.num_layers * (2 if self.bidrectional else 1), d_hidden * self.num_layers * (2 if self.bidrectional else 1)),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(d_hidden * self.num_layers * (2 if self.bidrectional else 1), d_out),\n",
        "        )\n",
        "\n",
        "        #### STUDENT CODE ENDS HERE ####\n",
        "\n",
        "    def forward(self, text, mask, seq_lengths):\n",
        "\n",
        "        batch_size = text.size()[0]\n",
        "\n",
        "        #### STUDENT CODE STARTS HERE ####\n",
        "        #compute the forward pass,\n",
        "        #take care to extract the appropriate parts of the rnn computation based on the length of the sentences\n",
        "\n",
        "        x = text\n",
        "        x_cpu = x.cpu()\n",
        "        lengths = np.count_nonzero(x_cpu, axis=1)\n",
        "        x = self.embed(x)\n",
        "        packed = torch.nn.utils.rnn.pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n",
        "        packed_output, hn = self.rnn(packed)\n",
        "        hn = torch.permute(hn, (1, 2, 0))\n",
        "        hn = torch.flatten(hn, start_dim=1)\n",
        "        output = self.feedforward_layers(hn)\n",
        "        m = nn.Softmax()\n",
        "        output = m(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "        #### STUDENT CODE ENDS HERE ####\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1234)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "batch_size = 256\n",
        "epochs = 5\n",
        "dev_every = 100\n",
        "lr = 0.0001\n",
        "save_path = \"gru_best_model\"\n",
        "drop_out = 0.3\n",
        "word_dropout = 0.25\n",
        "weight_decay = 0.001\n",
        "\n",
        "model = GRU_Classifier(n_embed=n_embed, d_embed=d_embed, d_hidden=300, d_out=d_out)\n",
        "dev_value, test_y_pred = train(model, lr, drop_out, word_dropout, batch_size, weight_decay, \"LSTM\")"
      ],
      "metadata": {
        "id": "1i09Uhz_-_nV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0fcb546b-9bf1-4670-8877-2bc9dea870e7"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch |   %        |  loss  |  avg   |val loss|   acc   |  best  | time | save |\n",
            "val   |            |        |        | 0.6927 | 56.7570 | 56.7570 |      |      |\n",
            "    0 | 25600/98794 | 0.4809 | 0.5123 | 0.4695 | 83.6555 | 83.6555 | 0.41 | *    |\n",
            "    0 | 51200/98794 | 0.4439 | 0.4609 | 0.4584 | 85.0555 | 85.0555 | 0.77 | *    |\n",
            "    0 | 76800/98794 | 0.5002 | 0.4588 | 0.4661 | 84.4772 | 85.0555 | 1.09 | *    |\n",
            "    1 | 3584/98794 | 0.3805 | 0.0150 | 0.4457 | 86.0676 | 86.0676 | 1.44 | *    |\n",
            "    1 | 29184/98794 | 0.4129 | 0.4296 | 0.4414 | 86.4328 | 86.4328 | 1.86 | *    |\n",
            "    1 | 54784/98794 | 0.4541 | 0.4275 | 0.4397 | 86.6535 | 86.6535 | 2.21 | *    |\n",
            "    1 | 80384/98794 | 0.4127 | 0.4231 | 0.4419 | 86.4937 | 86.6535 | 2.54 | *    |\n",
            "    2 | 7168/98794 | 0.4206 | 0.0146 | 0.4396 | 86.4937 | 86.6535 | 2.86 | *    |\n",
            "    2 | 32768/98794 | 0.4415 | 0.4143 | 0.4382 | 86.7448 | 86.7448 | 3.22 | *    |\n",
            "    2 | 58368/98794 | 0.3919 | 0.4138 | 0.4355 | 87.1861 | 87.1861 | 3.57 | *    |\n",
            "    2 | 83968/98794 | 0.4055 | 0.4115 | 0.4382 | 86.8970 | 87.1861 | 3.90 | *    |\n",
            "    3 | 10752/98794 | 0.4304 | 0.0141 | 0.4345 | 87.2698 | 87.2698 | 4.25 | *    |\n",
            "    3 | 36352/98794 | 0.4110 | 0.4018 | 0.4362 | 86.8665 | 87.2698 | 4.57 | *    |\n",
            "    3 | 61952/98794 | 0.4307 | 0.3987 | 0.4371 | 87.0948 | 87.2698 | 4.89 | *    |\n",
            "    3 | 87552/98794 | 0.4067 | 0.4030 | 0.4381 | 86.8741 | 87.2698 | 5.21 | *    |\n",
            "    4 | 14336/98794 | 0.4091 | 0.0137 | 0.4397 | 86.9807 | 87.2698 | 5.53 | *    |\n",
            "    4 | 39936/98794 | 0.3994 | 0.3905 | 0.4383 | 86.6306 | 87.2698 | 5.85 | *    |\n",
            "    4 | 65536/98794 | 0.3823 | 0.3953 | 0.4450 | 86.3263 | 87.2698 | 6.18 | *    |\n",
            "    4 | 91136/98794 | 0.3726 | 0.3935 | 0.4401 | 86.9731 | 87.2698 | 6.50 | *    |\n",
            "    4 | 98816/98794 | 0.4235 | 0.3913 |\n",
            "Validation Accuracy :  (87.26982194490945, 0.43446067)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random as random\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "from torch.nn import LSTM, GRU\n",
        "\n",
        "class RNN_Classifier(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                 n_embed=20000,\n",
        "                 d_embed=300,\n",
        "                 d_hidden=100,\n",
        "                 d_out=2,\n",
        "                 embeddings=None,\n",
        "                 nl = 20,\n",
        "                 bidirectional = False\n",
        "                 ):\n",
        "        super(RNN_Classifier, self).__init__()\n",
        "\n",
        "        self.d_hidden = d_hidden\n",
        "        self.bidrectional = bidirectional\n",
        "        self.num_layers = nl\n",
        "\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.embed = create_emb_layer(embedding_matrix,False)\n",
        "        #### STUDENT CODE STARTS HERE ####\n",
        "        #Define your rnn based archecture here\n",
        "\n",
        "        self.rnn = nn.RNN(input_size=d_embed, hidden_size=d_hidden,\n",
        "                           num_layers=self.num_layers, bias=True,\n",
        "                           batch_first=True, dropout=0.3,\n",
        "                           bidirectional=self.bidrectional,\n",
        "                           device=self.device)\n",
        "\n",
        "        self.feedforward_layers = nn.Sequential(\n",
        "            nn.Linear(d_hidden * self.num_layers * (2 if self.bidrectional else 1), d_hidden * self.num_layers * (2 if self.bidrectional else 1)),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(d_hidden * self.num_layers * (2 if self.bidrectional else 1), d_hidden * self.num_layers * (2 if self.bidrectional else 1)),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(d_hidden * self.num_layers * (2 if self.bidrectional else 1), d_hidden * self.num_layers * (2 if self.bidrectional else 1)),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(d_hidden * self.num_layers * (2 if self.bidrectional else 1), d_out),\n",
        "        )\n",
        "\n",
        "        #### STUDENT CODE ENDS HERE ####\n",
        "\n",
        "    def forward(self, text, mask, seq_lengths):\n",
        "\n",
        "        batch_size = text.size()[0]\n",
        "\n",
        "        #### STUDENT CODE STARTS HERE ####\n",
        "        #compute the forward pass,\n",
        "        #take care to extract the appropriate parts of the rnn computation based on the length of the sentences\n",
        "\n",
        "        x = text\n",
        "        x_cpu = x.cpu()\n",
        "        lengths = np.count_nonzero(x_cpu, axis=1)\n",
        "        x = self.embed(x)\n",
        "        packed = torch.nn.utils.rnn.pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n",
        "        packed_output, hn = self.rnn(packed)\n",
        "        hn = torch.permute(hn, (1, 2, 0))\n",
        "        hn = torch.flatten(hn, start_dim=1)\n",
        "        output = self.feedforward_layers(hn)\n",
        "        m = nn.Softmax()\n",
        "        output = m(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "        #### STUDENT CODE ENDS HERE ####\n"
      ],
      "metadata": {
        "id": "bUA8OCCeTE__"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1234)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "batch_size = 256\n",
        "epochs = 5\n",
        "dev_every = 100\n",
        "lr = 0.0001\n",
        "save_path = \"rnn_best_model\"\n",
        "drop_out = 0.3\n",
        "word_dropout = 0.25\n",
        "weight_decay = 0.001\n",
        "\n",
        "model = RNN_Classifier(n_embed=n_embed, d_embed=d_embed, d_hidden=300, d_out=d_out)\n",
        "dev_value, test_y_pred = train(model, lr, drop_out, word_dropout, batch_size, weight_decay, \"LSTM\")"
      ],
      "metadata": {
        "id": "8UVxRjRRTK8a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3523cb7-8430-47d3-ae16-38ed08ec5a8c"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch |   %        |  loss  |  avg   |val loss|   acc   |  best  | time | save |\n",
            "val   |            |        |        | 0.6928 | 56.7341 | 56.7341 |      |      |\n",
            "    0 | 25600/98794 | 0.4785 | 0.5190 | 0.4763 | 83.1532 | 83.1532 | 0.24 | *    |\n",
            "    0 | 51200/98794 | 0.4439 | 0.4714 | 0.4628 | 84.4620 | 84.4620 | 0.51 | *    |\n",
            "    0 | 76800/98794 | 0.4699 | 0.4537 | 0.4586 | 84.9795 | 84.9795 | 0.82 | *    |\n",
            "    1 | 3584/98794 | 0.3734 | 0.0152 | 0.4557 | 85.1088 | 85.1088 | 1.09 | *    |\n",
            "    1 | 29184/98794 | 0.4127 | 0.4322 | 0.4537 | 85.5654 | 85.5654 | 1.36 | *    |\n",
            "    1 | 54784/98794 | 0.4625 | 0.4303 | 0.4564 | 85.1164 | 85.5654 | 1.59 | *    |\n",
            "    1 | 80384/98794 | 0.4235 | 0.4269 | 0.4488 | 85.8545 | 85.8545 | 1.85 | *    |\n",
            "    2 | 7168/98794 | 0.4365 | 0.0145 | 0.4486 | 85.9610 | 85.9610 | 2.17 | *    |\n",
            "    2 | 32768/98794 | 0.4396 | 0.4194 | 0.4506 | 85.8393 | 85.9610 | 2.40 | *    |\n",
            "    2 | 58368/98794 | 0.3749 | 0.4166 | 0.4474 | 86.0752 | 86.0752 | 2.67 | *    |\n",
            "    2 | 83968/98794 | 0.4136 | 0.4106 | 0.4413 | 86.3948 | 86.3948 | 2.94 | *    |\n",
            "    3 | 10752/98794 | 0.4297 | 0.0143 | 0.4504 | 86.0828 | 86.3948 | 3.16 | *    |\n",
            "    3 | 36352/98794 | 0.4095 | 0.4052 | 0.4472 | 86.1741 | 86.3948 | 3.40 | *    |\n",
            "    3 | 61952/98794 | 0.4059 | 0.4046 | 0.4531 | 85.8241 | 86.3948 | 3.63 | *    |\n",
            "    3 | 87552/98794 | 0.4073 | 0.4062 | 0.4451 | 86.4024 | 86.4024 | 3.88 | *    |\n",
            "    4 | 14336/98794 | 0.4379 | 0.0140 | 0.4457 | 86.2806 | 86.4024 | 4.12 | *    |\n",
            "    4 | 39936/98794 | 0.4246 | 0.3946 | 0.4575 | 85.1316 | 86.4024 | 4.35 | *    |\n",
            "    4 | 65536/98794 | 0.4055 | 0.3996 | 0.4404 | 86.9122 | 86.9122 | 4.68 | *    |\n",
            "    4 | 91136/98794 | 0.3740 | 0.3988 | 0.4425 | 86.8057 | 86.9122 | 4.91 | *    |\n",
            "    4 | 98816/98794 | 0.4134 | 0.3966 |\n",
            "Validation Accuracy :  (86.91218992542991, 0.4403739)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mdux7pslH21n"
      },
      "source": [
        "## 5.0 Report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-a2lOUP7H5-Y"
      },
      "source": [
        "At this point, you should have built at least two types of models: an order agnostic model and at least two RNN based model. You will now need to experiment with different aspects of these models and discuss the results in your report. Go to the homework template, https://www.overleaf.com/read/sjtwwtnptzzx, make a copy of the latex file, and complete all sections and questions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYPEaqwucLzl"
      },
      "source": [
        "## Submission:\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcDVeSXHrwLg"
      },
      "source": [
        "These are the files you have to submit to gradescope\n",
        "\n",
        "1.   Report, with all sections answered\n",
        "2.   Download your jupyter notebook as a python file and submit it as \"homework3.ipynb\".\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}